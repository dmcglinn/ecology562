<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<title>Lecture 36&mdash;Wednesday, April 11, 2012</title>
<link rel="stylesheet" type="text/css" href="https://sakai.unc.edu/access/content/group/2842013b-58f5-4453-aa8d-3e01bacbfc3d/public/Ecol562_Spring2012/docs/lectures/css/green.css" title="green" /> 
<link rel="stylesheet" type="text/css" href="https://sakai.unc.edu/access/content/group/2842013b-58f5-4453-aa8d-3e01bacbfc3d/public/Ecol562_Spring2012/docs/lectures/css/calendar.css" title="calendar" /> 
<link rel="alternate stylesheet" type="text/css" media="all" href="https://sakai.unc.edu/access/content/group/2842013b-58f5-4453-aa8d-3e01bacbfc3d/public/Ecol562_Spring2012/docs/lectures/css/purple.css" title="purple" /> 
<link rel="alternate stylesheet" type="text/css" media="all" href="https://sakai.unc.edu/access/content/group/2842013b-58f5-4453-aa8d-3e01bacbfc3d/public/Ecol562_Spring2012/docs/lectures/css/large.css" title="large" /> 
<link rel="alternate stylesheet" type="text/css" media="all" href="https://sakai.unc.edu/access/content/group/2842013b-58f5-4453-aa8d-3e01bacbfc3d/public/Ecol562_Spring2012/docs/lectures/css/reverse.css" title="reverse" /> 
<!-- the @import method only works from 5.0 and upwards  -->
<!-- so, using @import would "hide" the more sophisticated sheet from < 5.0 browsers -->
<!-- <style type="text/css" media="all">@import "fancy_style.css";</style> -->
<script language="JavaScript" type="text/javascript" src="https://sakai.unc.edu/access/content/group/2842013b-58f5-4453-aa8d-3e01bacbfc3d/public/Ecol562_Spring2012/docs/lectures/js/styleswitcher.js"></script> 
<style type="text/css">
<!--
div.figure {float:none;width=25%;} 
div.figure p {test-aligh: center;font-style:italic;}
div.figureL {float:left;width=50%; margin:1.5em;padding:4px 4px 4px 0px;} 
div.figureL p {test-aligh: center;font-style:italic;}
div.figureR {float:right;width=50%;margin:1.5em;padding:4px 4px 4px 0px;} 
div.figureR p {test-aligh: center;font-style:italic;}

a:link {color: #0000CC; text-decoration:none}
a:visited {color: #0000CC; text-decoration:none}
a:hover {color: green; text-decoration:underline; background:#F9EDED}
a:active {color: red; text-decoration:none}

.style1 {
	color: #CC0000;
	font-weight: bold;
}
.style3 {
	color: #CC0000;
	font-weight: bold;
}
.style4 {color: #CCCCCC}
.style7 {font-family: "Courier New", Courier, mono}
.style8 {
	font-family: Arial, Helvetica, sans-serif;
	color: #810000;
}
.style9 {
	color: #3333CC;
	font-weight: bold;
}
.styleArial {
	font-family: Arial, Helvetica, sans-serif;font-size:11.0pt;
}
.style23 {
	font-family: "Courier New", Courier, mono;
	color: #000000;
	background-color:#F0F0F0;
}
.style11 {
	font-family: "Courier New", Courier, mono;
	color: #000000;
	
}

.style22 {color: #663366; font-weight: bold; }

.style10 {
	font-family: "Courier New", Courier, mono;
	color: #000000;
	background-color:#F0F0F0;
}

.style24 {
	font-family: "Courier New", Courier, mono;
	color: #0000FF;
	font-size:small;
}
.style25 {
	font-family: "Courier New", Courier, mono;
	color: #0000FF;
	background-color:#FFFC9A;
	font-size:small;
}
.style15 {font-family: "Courier New", Courier, mono; color: #339933; font-weight: bold; background-color:#F0F0F0; }
.style100 {
	background-color:#FFFC9A;
}



.style19 {color: #339933;
	font-weight: bold;}

.style42 {color: #0000FF; font-weight: bold; font-family: "Courier New", Courier, mono;  background-color:#F0F0F0;}
.style102 {color: #CC0000;
	font-weight: bold;
}
.style12 {color: #CC0000;
	font-weight: bold;
}
.style31 {color: #336699; font-weight: bold; }
.styleArial1 {	font-family: Arial, Helvetica, sans-serif;
	font-size:11.0pt;
}
.styleArial1 {	font-family: Arial, Helvetica, sans-serif;font-size:11.0pt;
}
.style242 {color: #009966;
	font-weight: bold;
}
.style13 {	color: #CC0000;
	font-weight: bold;
}
.style13 {	color: #CC0000;
	font-weight: bold;
}
.style1021 {color: #CC0000;
	font-weight: bold;
}
.style221 {color: #663366; font-weight: bold; }
.style221 {color: #990099;
	font-weight: bold;
}
.style41 {color: #CC0000;
	font-weight: bold;
}
.style44 {font-family: "Courier New", Courier, mono}
.style91 {color: #339900;
	font-weight: bold;
}
.style14 {font-family: "Courier New", Courier, mono;}

-->
</style>
</head>

<body>
<h1 align="center"><a name="lecture36" id="lecture36"></a>Lecture 36&mdash;Wednesday, April 11, 2012</h1>
<h2>Outline of lecture</h2>
<ul>
  <li><a href="lecture36.htm#cost">Cost complexity criterion</a></li>
  <li><a href="lecture36.htm#cross">Cross-validation</a></li>
  <li><a href="lecture36.htm#general">General comments about trees</a></li>
  <li><a href="lecture36.htm#modern">Extensions of regression trees&mdash;random forests</a>
    <ul>
      <li><a href="lecture36.htm#data">Sampling the data set</a></li>
      <li><a href="lecture36.htm#variables">Sampling the variables</a></li>
      <li><a href="lecture36.htm#overfitting">Overfitting the tree</a></li>
      <li><a href="lecture36.htm#summarizing">Summarizing the performance of a random forest</a></li>
      <li><a href="lecture36.htm#utility">The utility of random forests</a></li>
    </ul>
  </li>
  <li><a href="lecture36.htm#references">References</a></li>
</ul>
<h2><a name="cost"></a>The cost complexity criterion</h2>
<p>The basic method used in constructing a decision tree is an example of a greedy algorithm. In building the tree we choose each partition to obtain the best performance from the current tree  without regard to the performance of future trees. The implication of this is that the obvious stopping rule, keep going until the reduction in impurity fails to exceed a pre-determined minimum threshold value &epsilon;, is  a bad stopping rule. Because a greedy algorithm fails to look ahead, a worthless split at one stage can lead to a very worthwhile split at a later stage. In order to permit such worthwhile trees to arise a better strategy is to grow a very large tree and then prune it back. The standard criterion used in pruning trees is the cost complexity criterion <em>c<sub>p</sub></em>.</p>
<p>Suppose we grow a very large tree that has a total of <em>n</em> leaves. Assume that the tree is constructed as was described last time so that the best tree is found at each stage. If we  reverse things, the best tree at the previous stage is  the  one that can be obtained by combining adjacent nodes in such a way that impurity increases the least. Thus  pruning a tree retraces our steps and finds the best tree at each smaller size. Let <em>T</em> denote the current tree and let <img src="../../images/lectures/lecture36/absT.gif" alt="abs T" width="27" height="30" align="absmiddle">denote the number of splits that were used to create  this tree. The cost complexity (CC) function for this tree is defined as follows.</p>
<p align="center"><img src="../../images/lectures/lecture36/CC.gif" width="212" height="63" alt="CC"></p>
<p>Here <em>D<sub>i</sub></em> is the impurity at terminal node <em>i</em>: RSS for a continuous response or one of the categorical impurity measures defined last time. The parameter &lambda; is a penalty term. By varying &lambda; we can select different-sized  trees from our sequence of best trees. The principle behind the cost complexity function is comparable to the rationale for the formula for smoothing splines that is used in generalized additive models:</p>
<p align="center"><img src="../../images/lectures/lecture36/smoothspline.gif" width="288" height="50" alt="smoothing spline"></p>
<p>Here &lambda; plays a similar role as it does in the cost complexity function. </p>
<p>The <span class="style41">rpart</span> function of the <span class="style19">rpart</span> package of R does not report &lambda; from the complexity function, but rather a scaled version of &lambda; that it denotes as <em>c<sub>p</sub></em>, the cost complexity criterion.</p>
<p align="center"><img src="../../images/lectures/lecture36/cp.gif" width="163" height="57" alt="cp"></p>
<p>A  null tree is one with no splits. <span class="style41">rpart</span> displays the  sequence of best trees obtained as the value of <em>c<sub>p</sub></em> is varied.</p>
<h2><a name="cross"></a>Cross-validation</h2>
<p>So the problem of when to stop building a tree has been converted into a problem of when to stop pruning a tree. To answer this we turn to cross-validation. For each best tree of a fixed size (or alternatively for each  value of <em>c<sub>p</sub></em> yielding a different tree) we carry out a <em>k</em>-fold cross-validation. To do this we randomly divide the data <em>D</em> into <em>k</em> subsets such that </p>
<p align="center"><img src="../../images/lectures/lecture36/crossvalidation.gif" width="223" height="27" alt="cross validation"></p>
<p>Typically <em>k</em> is chosen to be 10. We leave out each one of the <em>k</em> subsets <img src="../../images/lectures/lecture36/Dsupi.gif" alt="Di" width="35" height="25" align="absmiddle"> in turn  and fit the tree to a data set consisting of the remaining <em>k</em> &ndash; 1 subsets combined. Fitting the tree means running the observations through the previously constructed tree branches to obtain the predictions at the leaves,<img src="../../images/lectures/lecture36/ybari.gif" alt="ybari" width="20" height="27" align="absmiddle"> or <img src="../../images/lectures/lecture36/pij.gif" alt="pij" width="25" height="30" align="absmiddle"> depending on whether it is a regression tree or a classification tree. The data that were left out are then run through the tree and are used to calculate the impurity of the tree at each node. This process is repeated <em>k</em> times, once for each subset, and the relative average cross-validation error (<span class="style14">xerror</span>) is calculated as follows.</p>
<p align="center"><img src="../../images/lectures/lecture36/xerror.gif" width="262" height="75" alt="xerror"></p>
<p><span class="style31"><strong>Stopping rule #1</strong>:</span> Choose the value <em>c<sub>p</sub></em> (or equivalently the number of splits) that produces a tree that minimizes cross-validation relative error <span class="style14">xerror</span>. The problem with this stopping rule is that <span class="style14">xerror</span> is random because it depends on the <em>k</em>-fold partition that was actually obtained. Thus a better rule is one that takes into account the variability of <span class="style14">xerror</span>.</p>
<p><span class="style31"><strong>Stopping rule #2</strong>:</span> Choose the first (largest) value of <em>c<sub>p</sub></em> such that <span class="style14"> xerror &lt; min(xerror) + xstd</span>. Here <span class="style14">xstd</span> is the estimate of the standard deviation of <span class="style14">xerror</span> that is calculated from the tree with the minimum value of <span class="style14">xerror</span>.</p>
<h2><a name="general"></a>General comments about trees</h2>
<ol>
  <li>Monotone transformations of the predictors have no effect on the tree one gets. This is not true for transformations of the response because a transformation will change the residual sum of squares.</li>
  <li>Trees are not useful with small data sets. To achieve any progress with small data sets one is typically forced to make very strong assumptions. Trees on the other hand make almost no assumptions at all.</li>
  <li>Trees are especially useful with large data sets in which there are many predictors to choose from. In this scenario  the weak assumptions made by trees are not limiting and good results can be obtained. Trees have become especially popular for data mining projects.</li>
  <li>Trees can find complex structure in data sets, structure that cannot be detected with ordinary regression models.</li>
  <li>Because of the way trees are formulated, the overall tree can be far from optimal even though  the tree obtained at each local step is optimal.</li>
  <li>Trees are not able to account for correlated data. Put another way trees built using correlated data only represent other similarly correlated data sets from that population.</li>
  <li>Trees deal with continuous variables by essentially converting them into categorical variables.</li>
</ol>
<p>How can trees be used in conjunction with ordinary parametric models?</p>
<ol>
  <li>Trees can be used to suggest possible interactions. When two different variables are used to split a tree at different points along the same branch, one interpretation is that there is  an interaction between those two variables.</li>
  <li>If the same variable appears repeatedly along the same branch of a tree, it may mean there is a nonlinear relationship between that variable and the response.</li>
  <li>Having estimated a parametric model, one can run the residuals from the model through a tree to see if there is any additional structure that has been overlooked.</li>
  <li>One can take the fitted values from a parametric regression model and use them to depict the regression model as a tree with a series of decision rules.</li>
</ol>
<h2><a name="modern"></a>Extensions of  regression trees&mdash;random forests</h2>
<p>Regression trees have come a long way in the 25 years since their inception. Recent attention has focused  on what are called ensemble methods. These are associated with colorful names such as boosting, bagging, and random forests. The basic idea behind all of these methods is to grow trees on perturbed instances of the data, each time obtaining a set of predictions and then averaging the predictions over repeated samples. Ensemble methods try to address two problems that can arise with individual decision trees.</p>
<ol>
  <li>Decision trees tend to not  be very robust. Removing a fraction of the data can change an individual tree dramatically.</li>
  <li>Even with pruning individual trees tend to overfit to particular data sets.</li>
</ol>
<p>Random forests get around both problems by interjecting randomness into the process.  This is done by including random sampling at two stages of the tree-building process.</p>
<h3><a name="data" id="data"></a>Sampling the data set</h3>
<p>In the random forest implementation that is available for  R, bootstrap samples (500 by default) are drawn from the raw data and are used to grow trees. So each of the 500 trees is built from a different set of observations. A bootstrap sample is a random sample with replacement so the same observations can appear in a sample multiple times. Typically with a large number of observations 2/3 of them will appear in a given bootstrap sample and 1/3 will be left out. The machine learning terminology for this is &quot;bagging&quot;, an acronym for &quot;bootstrap aggregation&quot;. Those observations selected for the sample are said to be  &quot;in  the bag&quot;  and those left out are said to be   &quot;out of bag&quot;. The in the bag observations serve as the training data set for building the tree. The out of bag observations are used as the test data set for determining how well the tree predicts new observations.</p>
<h3><a name="variables"></a>Sampling the variables</h3>
<p>Instead of using all the available variables to define a split at a decision node,  a small random sample of predictors is selected  and evaluated for split points. Thus within a given tree the variables that are available for use at each split point will vary depending on the random sample  obtained. In R if there are <em>p</em> predictors available in the data set, a random sample of size  <img src="../../images/lectures/lecture36/sqrtp.gif" alt="sqrt(p)" width="33" height="33" align="absmiddle"> of these predictors is selected at each split point for a classification tree. For regression trees the default is 1/3 of the predictors. In addition to introducing variability into the trees, working with a smaller  predictor set at each decision node reduces the computation load.</p>
<h3><a name="overfitting"></a>Overfitting the tree</h3>
<p>Given the different data sets and different predictor sets we expect the performance of the different trees to vary. Each tree is built to its maximum depth with no pruning so that the tree performs well (has minimum bias) on the data set that was used to build it. Although each tree overfits to its particular data set, over the entire ensemble of trees each tree is overfitting differently because the variables are different (as a result of the random selection of variables at nodes) and the data sets are different (as a result of bagging). The individual trees are analogous to a set of experts each of which has a different and very specific expertise but collectively they encompass a wide range of knowledge. As a result an ensemble of trees tends to perform well with new data.</p>
<h3><a name="summarizing"></a>Summarizing the performance of a random forest</h3>
<p>Random forests use ensemble scoring. Predictions are calculated as  the average of the predictions of the individual trees. Because so many trees are involved describing the fit of a random forest  is more complicated than it is with a single decision tree. </p>
<p>One  statistic reported by a random forest is the OOB (out of bag) estimate of the error rate. This is calculated using the out of bag observations. Because   bootstrap samples were used to build each tree, each observation will  on average be OOB in about 1/3 of the trees. For each tree in which the observation is OOB we can calculate the prediction error.  This error then gets averaged across all trees for which that observation is OOB. For classification trees the prediction error is the proportion of  trees for which the OOB observation is misclassified. Finally we average this over all observations to obtain a single summary statistic. The OOB estimate of the error rate tells us how well the forest  will generalize to new data.</p>
<p>A random forest also reports the importance of each variable in the forest. This is calculated in a number of different ways. The Gini importance of a variable is the average decrease in the Gini index whenever that variable is used to make a decision at a split point. Another importance measure is the scaled average of prediction accuracy. For this each variable has its values  randomly permuted among the other OOB observations. The predictions for these  observations after permutation are obtained and compared against the the predictions before permutation.  The average decrease in accuracy is then calculated. The larger this is the more important is the variable. The importance values of different variables are typically displayed in a dot plot in which the variables are ranked by their importance.</p>
<p> In classification trees we may  wish to treat false positive errors differently from false negative errors. There are tuning parameters (the <span class="style22">sampsize</span> argument in the R implementation of <span class="style1">randomForest</span> for example) that can be used to force a tree to take one of the errors more seriously than the other. This is analogous to adjusting the probability cut-off in a decision rule  in order to change the sensitivity and specificity of logistic regression models.</p>
<h3><a name="utility"></a>The utility of random forests</h3>
<p>Random forests are ensembles of hundreds of unpruned decision trees. They are typically used with large data sets that have hundreds or even thousands of input variables. It is even possible to use random forests when there are more variables than there are observations. Unlike single trees, random forests are not sensitive to noise. Because the individual trees making up the forest are built to maximum depth, they tend to have low bias. Classification trees are especially good at handling under-represented classes. </p>
<p>The process of  bagging and the use of small random samples of predictors at each node protects random forests against overfitting and guards against the undue influence of outliers. This built-in randomness also makes each tree in the ensemble a more or less independent model. For a recent introduction to random forests for ecologists see Cutler et al. (2007).</p>
<h2><a name="references" id="references"></a>References</h2>
<p align="left">Cutler, D. R., T. C. Edwards, K. H. Beard, A. Cutler, K. T. Hess, J. Gibson, and J. J. Lawler. 2007. Random forests for classification in ecology. <em>Ecology</em> <strong>88</strong>: 2783&ndash;2792.</p>
<p align="center"><a href="../../index.html">Course Home Page</a> </p>
<hr align="center" width="75%">
<!--Standard footer follows -->
<p></p>
<table width="650" border="3" cellspacing="2" cellpadding="2" align=
"CENTER">
  <tr bgcolor="#CCCCCC">
    <td width="100%"><font size=-1>Jack Weiss<br>
          <i>Phone: </i>(919) 962-5930<br>
          <i>E-Mail:</i> jack_weiss@unc.edu<br>
          <i>Address: </i>Curriculum for the Environment and Ecology, Box 3275, University of North Carolina, Chapel Hill, 27599<br>
      Copyright &copy; 2012<br>
      Last Revised--April 12, 2012<br>
      URL: <a href="lecture36.htm#lecture36" target="_self">https://sakai.unc.edu/access/content/group/2842013b-58f5-4453-aa8d-3e01bacbfc3d/public/Ecol562_Spring2012/docs/lectures/lecture36.htm</a></font></td>
  </tr>
</table>
<p align="center">&nbsp;</p>
</body>
</html>
