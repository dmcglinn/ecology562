<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<title>Lecture 35&mdash;Monday, April 9, 2012</title>
<link rel="stylesheet" type="text/css" href="https://sakai.unc.edu/access/content/group/2842013b-58f5-4453-aa8d-3e01bacbfc3d/public/Ecol562_Spring2012/docs/lectures/css/green.css" title="green" /> 
<link rel="stylesheet" type="text/css" href="https://sakai.unc.edu/access/content/group/2842013b-58f5-4453-aa8d-3e01bacbfc3d/public/Ecol562_Spring2012/docs/lectures/css/calendar.css" title="calendar" /> 
<link rel="alternate stylesheet" type="text/css" media="all" href="https://sakai.unc.edu/access/content/group/2842013b-58f5-4453-aa8d-3e01bacbfc3d/public/Ecol562_Spring2012/docs/lectures/css/purple.css" title="purple" /> 
<link rel="alternate stylesheet" type="text/css" media="all" href="https://sakai.unc.edu/access/content/group/2842013b-58f5-4453-aa8d-3e01bacbfc3d/public/Ecol562_Spring2012/docs/lectures/css/large.css" title="large" /> 
<link rel="alternate stylesheet" type="text/css" media="all" href="https://sakai.unc.edu/access/content/group/2842013b-58f5-4453-aa8d-3e01bacbfc3d/public/Ecol562_Spring2012/docs/lectures/css/reverse.css" title="reverse" /> 
<!-- the @import method only works from 5.0 and upwards  -->
<!-- so, using @import would "hide" the more sophisticated sheet from < 5.0 browsers -->
<!-- <style type="text/css" media="all">@import "fancy_style.css";</style> -->
<script language="JavaScript" type="text/javascript" src="https://sakai.unc.edu/access/content/group/2842013b-58f5-4453-aa8d-3e01bacbfc3d/public/Ecol562_Spring2012/docs/lectures/js/styleswitcher.js"></script> 
<style type="text/css">
<!--
div.figure {float:none;width=25%;} 
div.figure p {test-aligh: center;font-style:italic;}
div.figureL {float:left;width=50%; margin:1.5em;padding:4px 4px 4px 0px;} 
div.figureL p {test-aligh: center;font-style:italic;}
div.figureR {float:right;width=50%;margin:1.5em;padding:4px 4px 4px 0px;} 
div.figureR p {test-aligh: center;font-style:italic;}

a:link {color: #0000CC; text-decoration:none}
a:visited {color: #0000CC; text-decoration:none}
a:hover {color: green; text-decoration:underline; background:#F9EDED}
a:active {color: red; text-decoration:none}

.style1 {
	color: #CC0000;
	font-weight: bold;
}
.style3 {
	color: #CC0000;
	font-weight: bold;
}
.style4 {color: #CCCCCC}
.style7 {font-family: "Courier New", Courier, mono}
.style8 {
	font-family: Arial, Helvetica, sans-serif;
	color: #810000;
}
.style9 {
	color: #3333CC;
	font-weight: bold;
}
.styleArial {
	font-family: Arial, Helvetica, sans-serif;font-size:11.0pt;
}
.style23 {
	font-family: "Courier New", Courier, mono;
	color: #000000;
	background-color:#F0F0F0;
}
.style11 {
	font-family: "Courier New", Courier, mono;
	color: #000000;
	
}

.style22 {color: #663366; font-weight: bold; }

.style10 {
	font-family: "Courier New", Courier, mono;
	color: #000000;
	background-color:#F0F0F0;
}

.style24 {
	font-family: "Courier New", Courier, mono;
	color: #0000FF;
	font-size:small;
}
.style25 {
	font-family: "Courier New", Courier, mono;
	color: #0000FF;
	background-color:#FFFC9A;
	font-size:small;
}
.style15 {font-family: "Courier New", Courier, mono; color: #339933; font-weight: bold; background-color:#F0F0F0; }
.style100 {
	background-color:#FFFC9A;
}



.style19 {color: #339933;
	font-weight: bold;}

.style42 {color: #0000FF; font-weight: bold; font-family: "Courier New", Courier, mono;  background-color:#F0F0F0;}
.style102 {color: #CC0000;
	font-weight: bold;
}
.style12 {color: #CC0000;
	font-weight: bold;
}
.style31 {color: #336699; font-weight: bold; }
.styleArial1 {	font-family: Arial, Helvetica, sans-serif;
	font-size:11.0pt;
}
.styleArial1 {	font-family: Arial, Helvetica, sans-serif;font-size:11.0pt;
}
.style242 {color: #009966;
	font-weight: bold;
}
.style13 {	color: #CC0000;
	font-weight: bold;
}
.style13 {	color: #CC0000;
	font-weight: bold;
}
.style1021 {color: #CC0000;
	font-weight: bold;
}
.style221 {color: #663366; font-weight: bold; }
.style221 {color: #990099;
	font-weight: bold;
}
.style41 {color: #CC0000;
	font-weight: bold;
}
.style44 {font-family: "Courier New", Courier, mono}
.style91 {color: #339900;
	font-weight: bold;
}

-->
</style>
</head>

<body>
<h1 align="center"><a name="lecture35" id="lecture35"></a>Lecture 35&mdash;Monday, April 9, 2012</h1>
<h2>Outline of lecture</h2>
<ul>
  <li><a href="lecture35.htm#model">Model-based and algorithm-based approaches to statistics</a></li>
  <li><a href="lecture35.htm#binary">Binary recursive partitioning&mdash;heuristic description</a></li>
  <li><a href="lecture35.htm#recursive">The recursive partitioning algorithm&mdash;formal description</a></li>
  <li><a href="lecture35.htm#references">References</a></li>
  <li><a href="lecture35.htm#rcode">R code used to produce figures</a></li>
</ul>
<h2 align="left"><strong><a name="model"></a>Model-based and algorithm-based approaches to statistics</strong></h2>
<p>So far in this course we've focused exclusively on what is called a model-based approach to statistics. In statistics a model is a proposal for how we think our data could have been generated. A statistical model contains both a systematic and a random component each of which has to specified by us. Model-based approaches while intuitively appealing have some obvious problems associated with them.</p>
<ol>
  <li>Somehow we have to come up with the model. While the choice of  random component may be more or less obvious given the nature of the data, we still need to decide which variables to use to describe the systematic portion of the model.</li>
  <li>Variable selection itself is very model-specific.
    <ol type="a">
      <li> If we use linear regression, then we will end up selecting only those variables that have an additive and a linear relationship to the response.</li>
      <li>If we start with a log-transformed response and use linear regression, then we will select those variables that have an additive and a linear relationship to the response on a log scale. On the original scale, these same variables will necessarily have a multiplicative effect on the response.</li>
    </ol>
  </li>
  <li>Nonlinear relationships complicate things further as the number of possible functional forms to investigate is endless. These range from  truly nonlinear models to nominally nonlinear models, such as polynomial models, that can still be fit as ordinary regression models. If we use a generalized additive model (GAM) then we can address the general problem of nonlinearity without worrying about the specific functional form, but   we are still forced to assume that the different variables have an additive effect on the response.</li>
  <li>All of this ignores the possibility  that the variables affect the response interactively. The presence of interactions complicates model formulation by an order of magnitude.</li>
</ol>
<p>There is another approach to statistical modeling in which the actual model itself is secondary. Instead the  algorithm used to obtain the model takes center stage. In a regression setting, this algorithm-based approach  is generally some variation of what's called recursive binary partitioning (implemented in the <span class="style19">rpart</span> package of R). It is also called CART, an acronym for classification and regression trees, but this is also the name of a commercial product developed by one of the pioneers in this area, Leo Breiman (<a href="lecture35.htm#breiman">Breiman et al. 1984</a>).</p>
<p>Binary recursive partitioning is typically lumped into a category of methods that are referred to as &quot;machine learning&quot; or &quot;statistical learning&quot;  methods. Members of this group include support vector machines,  neural networks, and many others. These methods have virtually nothing in common with each other except for the fact that they originated in the field of computer science rather than statistics. In some cases  computer scientists  reinvented methodologies that already had a long history in statistics. In doing so they managed to create their own terminology  leaving us today with different descriptions of otherwise identical methods. As a result much of modern statistical learning theory is an awkward amalgamation of computer science and statistics. Good  introductions to this area include <a href="lecture35.htm#berk">Berk (2008)</a> and <a href="lecture35.htm#hastie">Hastie et al. (2009)</a>. Introductory treatments for ecologists are <a href="lecture35.htm#fielding">Fielding (1999)</a> and <a href="lecture35.htm#olden">Olden et al. (2008)</a>.</p>
<h2><a name="binary"></a>Binary recursive partitioning&mdash;heuristic description</h2>
<p>In binary recursive partitioning the goal is to partition the predictor space into boxes and then assign a numerical value to each box based on  the values of the response variable for the observations assigned to that box. At each step of the partitioning process we   are required to choose a specific variable and a split point for that variable that we then use  to divide all or a portion of the data set into two groups.  This is done by selecting a group to divide and then examining all possible variables and all possible split points of those variables. Having selected the combination of group, variable, and split point that yields the greatest improvement in the fit criterion we are using, we then divide that group into two parts. This process then repeats.</p>
<p>For a continuous response the object that is created is called a regression tree, whereas if the response is a categorical variable it's called a classification tree. Classification and regression trees are collectively referred to as decision trees. The usual fit criterion for a regression tree is the residual sum of squares, RSS. The construction of a generic regression tree model would proceed as follows.</p>
<ol>
  <li>Initially the RSS is the sum of squared deviations of the individual values of the response variable about their overall mean. This is called the null (root) RSS.</li>
  <li>Next examine each variable and each possible split point for dividing the data into two regions. For each choice calculate the mean of the response variable and the residual sum of squares in  each region separately. Sum them to obtain the total residual sum of squares for this partition and compare it to the null RSS. Identify the variable and split point that yields the greatest reduction in the RSS from the null model.</li>
  <li>For all the individual regions created so far, examine each variable and possible split point again to see if one of the current regions can be divided further. For each new division calculate the mean of the response variable and  the new RSS. Sum the RSS from all regions, new and old, to obtain the RSS for this partition. Compare it to the previous RSS and choose the variable,  split point, and region that yields the greatest reduction in the RSS.</li>
  <li>Repeat step 3  until the stopping criterion is met.</li>
</ol>
<p>Fig. 1 illustrates a toy example of this process with only two variables X<sub>1</sub> and X<sub>2</sub>. The rectangle shown in each panel is meant to delimit the range of the two variables. Here's a possible scenario for how the regression tree might evolve.</p>
<ol type=a>
  <li>At the first step using the RSS criterion we choose to split the X<sub>1</sub> axis into two parts at X<sub>1</sub> = t<sub>1</sub> yielding  Fig. 1b. </li>
  <li>We next examine the two rectangles just created in turn and find that the next best split is obtained using X<sub>2</sub> = t<sub>2</sub> . This splits the left hand rectangle into two parts, Fig. 1c. </li>
  <li>We now have three rectangles to examine and we find that splitting the right hand rectangle at X<sub>1</sub> = t<sub>3</sub> yields the optimal split, Fig. 1d. </li>
  <li>Of the four rectangles remaining we find that the optimal next split is of the right hand rectangle using X<sub>2</sub> = t<sub>4</sub> yielding the five rectangles shown in Fig. 1e. </li>
  <li>At this point our stopping criterion is met and the algorithm terminates.</li>
</ol>
<p align="center"><img src="../../images/lectures/lecture35/fig1a.png" width="729" height="388" alt="fig. 1"></p>
<p align="center" class="styleArial"><strong>Fig. 1</strong> &nbsp;Heuristic example of binary recursive partitioning with a predictor set consisting of two variables X<sub>1</sub> and X<sub>2</sub></p>
<p>This yields the five rectangular regions R<sub>1</sub>, R<sub>2</sub>, R<sub>3</sub>, R<sub>4</sub>, and R<sub>5</sub> shown in Fig. 1e and Fig. 2a. Next we summarize the response variable by its  mean, <img src="../../images/lectures/lecture35/ybar1.gif" width="20" height="27" align="absmiddle">, <img src="../../images/lectures/lecture35/ybar2.gif" width="22" height="27" align="absmiddle">, <img src="../../images/lectures/lecture35/ybar3.gif" alt="ybar3" width="22" height="27" align="absmiddle">, <img src="../../images/lectures/lecture35/ybar4.gif" alt="ybar4" width="23" height="27" align="absmiddle">, and <img src="../../images/lectures/lecture35/ybar5.gif" alt="ybar5" width="22" height="27" align="absmiddle">, in each region. In an ordinary regression model with two variables, the regression model is visualized as a surface lying over the X<sub>1</sub>-X<sub>2</sub> plane. For a  model linear in X<sub>1</sub> and X<sub>2</sub>, the surface is a plane. If the model contains a cross-product term X<sub>1</sub>X<sub>2</sub>, then the surface is curved. The model underlying a binary recursive partition can be visualized as a step surface such as the one shown in Fig. 2b. Over each of the regions shown in Fig. 2a we obtain a rectangular box whose cross-section is the given rectangular region and whose height is the mean of the response variable in that region. </p>
<table width="625" border="0" align="center">
  <tr valign="top">
    <td><div align="center">(a) <img src="../../images/lectures/lecture35/fig2a.png" alt="Fig. 2a" width="220" height="200" align="texttop"></div></td>
    <td><div align="center">(b) <img src="../../images/lectures/lecture35/fig2b.png" width="230" height="217" align="top"></div></td>
  </tr>
  <tr>
    <td colspan="2" class="styleArial"><p style="padding-left: 46px; text-indent:-46px"><strong>Fig. 2</strong> (a)  The rectangular regions in the X<sub>1</sub>-X<sub>2</sub> plane that were defined by the binary recursive partition shown in Fig. 1.  (b)  The regression surface for the response that corresponds to this binary recursive partition. Each box in (b) lies on one of the rectangular regions in (a) and its height corresponds to the mean of the response variable over that region.</td>
  </tr>
</table>
<p>The formula for the regression surface in Fig. 2b is the following.</p>
<p align="center"><img src="../../images/lectures/lecture35/rpart&#32;formula.gif" width="233" height="58" alt="rpartformula"></p>
<p>Here <em>I</em> is an indicator function that indicates whether the  observation lies in the given rectangular region. Because an observation can only occur in one of the five regions, four of the terms in this sum are necessarily zero for each observation. As a result we just get <img src="../../images/lectures/lecture35/ybar.gif" alt="ybar" width="18" height="25" align="absmiddle"> for the region that contains the point.</p>
<p>Because at each stage of the recursive partition we are only allowed to divide previously split single regions, the algorithm itself can be visualized as a tree. Fig. 3a shows the tree that corresponds to the regions defined in Fig. 2a. The splitting rules used at the various steps of the algorithm are given at the branch points. By following the  branches of the tree as dictated by splitting rules we arrive at the rectangular regions shown in Fig. 2a.</p>
<table width="650" border="0" align="center">
  <tr valign="top">
    <td><div align="center">(a) <img src="../../images/lectures/lecture35/fig1f.png" alt="Fig. 2b" width="362" height="212" align="top"></div></td>
    <td>(b) <img src="../../images/lectures/lecture35/fig3b.png" alt="fig 3b" width="220" height="200" align="top"></td>
  </tr>
  <tr>
    <td colspan="2"><p span style="padding-left: 44px; text-indent:-44px"><span class="styleArial"><strong>Fig. 3</strong> &nbsp;(a) When we focus on the algorithm itself binary recursive partitioning yields a binary tree. (b) The terminology associated with regression and classification trees</span></td>
  </tr>
</table>
<p>The branching points are called splits and branch endpoints are called leaves (or terminal nodes). Both the splits and the leaves are referred to as nodes (Fig 3b). In the heuristic tree of Fig. 3b there are four splits and five leaves for a total of nine nodes.</p>
<h2><a name="recursive"></a>The recursive partitioning algorithm&mdash;formal description</h2>
<p><strong>Step 1: Choose a variable and a split point.</strong></p>
<p>For a continuous or ordinal variable with <em>m</em> distinct values,  &xi;<sub>1</sub>, &xi;<sub>2</sub>, &#8230;, &xi;<sub><em>m</em></sub>, consider each value in turn. Each selection produces a partition consisting of two sets of values: <img src="../../images/lectures/lecture35/set1.gif" width="70" height="32" align="absmiddle"> and <img src="../../images/lectures/lecture35/set2.gif" alt="set2" width="70" height="32" align="absmiddle">, <em>i</em> = 1, 2, &#8230; , <em>m</em>. Splits can only occur between consecutive data values. Thus there are <em>m</em>&ndash;1 possible partitions to consider. For a categorical (nominal) variable with <em>m</em> categories we have to consider all possible ways of assigning the categories to two groups. The splits can be anywhere and  any category can be assigned to any group. Combinatorial arguments tell us that there are <img src="../../images/lectures/lecture35/catgroups.gif" alt="cat groups" width="67" height="25" align="absmiddle"> ways in which this can be done. (With <em>m</em> categories there are 2<sup><em>m</em></sup> different subsets. Because the complement of each subset is another subset this corresponds to 2<sup><em>m</em>&ndash;1</sup> splits. One of the subsets includes everybody (or nobody) which is not a split so we reduce this number further by 1.)</p>
<p><strong>Step 2: For each possible division calculate its impurity.</strong></p>
<p>Different measures of impurity are used for continuous and categorical predictors.</p>
<p><strong class="style31">Continuous response:</strong> For a continuous response <em>y</em>, suppose we select variable X<sub>j</sub> and split point <em>s</em> that yield the following two groups.</p>
<p align="center"><img src="../../images/lectures/lecture35/R1.gif" alt="R1" width="178" height="40" align="absmiddle"> and <img src="../../images/lectures/lecture35/R2.gif" alt="R2" width="182" height="40" align="absmiddle"></p>
<p>The usual measure of impurity used for continuous variables is the residual sum of squares. For the observations assigned to each group we calculate the mean of the response variable. The residuals sum of squares, RSS, for the current split is defined as follows.</p>
<p align="center"><img src="../../images/lectures/lecture35/RSSsplit.gif" width="325" height="83" alt="RSS split"></p>
<p>Before the data are split we  calculate the overall residual sum of squares using the mean of all values of the response variable.</p>
<p align="center"><img src="../../images/lectures/lecture35/RSSnull.gif" width="162" height="37" alt="RSS null"></p>
<p>So for a continuous response we examine each variable <em>X<sub>j</sub></em> and all possible split points <em>s</em> and choose the one for which RSS<sub>0</sub> &ndash; RSS(split) is the largest.</p>
<p><strong class="style31">Categorical response:</strong> For categorical variables there are a number of different ways of calculating impurity. Let <em>y</em> be a categorical variable with <em>m</em> categories. Let</p>
<blockquote>
  <p><em>n<sub>ik</sub></em> = number of observations of type <em>k</em> at node <em>i</em></p>
  <p><em>p<sub>ik</sub></em> = proportion of observations of type <em>k</em> at node <em>i</em></p>
</blockquote>
<p>The following four measures of impurity are commonly used.</p>
<ol>
  <li>Deviance: <img src="../../images/lectures/lecture35/deviance.gif" alt="deviance" width="165" height="47" align="absmiddle"></li>
  <li>Entropy: <img src="../../images/lectures/lecture35/entropy.gif" alt="entropy" width="168" height="47" align="absmiddle"></li>
  <li>Gini index: <img src="../../images/lectures/lecture35/gini.gif" alt="gini index" width="120" height="47" align="absmiddle"></li>
  <li>Misclassification error: <img src="../../images/lectures/lecture35/misclass.gif" alt="misclassification" width="108" height="30" align="absmiddle"> where <em>k</em>(<em>i</em>) is the category at node <em>i</em> with the largest number of observations.</li>
</ol>
<p>The impurity for a given partition is the sum of the impurities over all nodes, <img src="../../images/lectures/lecture35/impurity.gif" alt="impurity" width="50" height="47" align="absmiddle">. For a categorical variable all four measures of impurity  are minimized when all the observations at a node are of the same type. The goal in a classification tree is to choose splits that yield groups that are purer than in the current partition.</p>
<p><strong>Step 3: Try to partition the current groups further.</strong> Having chosen a best partition at the current stage of the tree, we next try to partition each of the groups that were formed by considering all nodes in turn. We are only allowed to partition within existing partitions (not across partitions). It is adherence to this rule that produces the tree structure.</p>
<p><strong>Step 4: At some point  stop.</strong> The issue of stopping rules is discussed next time.</p>
<h2><a name="references" id="references"></a>References</h2>
<ul>
  <li><a name="berk"></a>Berk, Richard A. 2008. <em>Statistical Learning from a Regression Perspective</em>. New York: Springer-Verlag.</li>
  <li><a name="breiman"></a>Breiman, Leo, Jerome Friedman, Richard A. Olshen, and Charles J. Stone. 1984. <em>Classification and Regression Trees</em>. Boca Raton, FL: Chapman &amp; Hall. </li>
  <li><a name="fielding"></a>Fielding, Alan. 1999. <em>Machine Learning Methods for Ecological Applications</em>.  Norwell, MA:  Kluwer Academic Publishers.</li>
  <li><a name="hastie"></a>Hastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2009. <em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction.</em> New York: Springer-Verlag.</li>
  <li><a name="olden"></a>Olden, J. D., J. J. Lawler, and N. L. Poff. 2008. Machine learning methods without tears: a primer for ecologists. <em>Quarterly Review of Biology</em> <strong>83</strong>(2): 171&ndash;193.<br>
</li>
</ul>
<h2><a name="rcode"></a>R code for figures</h2>
<div  class="style44" style="padding-left: 30px; text-indent:-30px"><span class="style91">#Fig. 1a</span></div>
<div class="style44" style="padding-left: 30px; text-indent:-30px">plot(0:7, 0:7, type='n', axes=F, xlab='', ylab='')</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> rect(1,1,6,6)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> text(3.5,.5, expression(X[1]), pos=1, cex=1.1)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px">text(.65,3.5, expression(X[2]), pos=2, cex=1.1)</div>
<div  class="style44" style="padding-left: 30px; text-indent:-30px"><span class="style91">#Fig. 1b</span></div>
<div class="style44" style="padding-left: 30px; text-indent:-30px">plot(0:7,0:7, type='n', axes=F, xlab='', ylab='')</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> rect(1,1,6,6)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> text(3.5,.5, expression(X[1]), pos=1, cex=1.1)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> text(.65,3.5, expression(X[2]), pos=2, cex=1.1)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> segments(2.75,1,2.75,6, col=2, lty=2, lwd=2)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> text(2.75,1, expression(t[1]), pos=1, col=2, cex=1)</div>
<div  class="style44" style="padding-left: 30px; text-indent:-30px"><span class="style91">#Fig. 1c</span></div>
<div class="style44" style="padding-left: 30px; text-indent:-30px">plot(0:7,0:7, type='n', axes=F, xlab='', ylab='')</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> rect(1,1,6,6)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> text(3.5,.5, expression(X[1]), pos=1, cex=1.1)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> text(.65,3.5, expression(X[2]), pos=2, cex=1.1)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> segments(2.75,1,2.75,6, col=2, lty=2, lwd=2)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> text(2.75,1, expression(t[1]), pos=1, col=2, cex=1)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> segments(1,4.5,2.75,4.5, col=2, lty=2, lwd=2)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> text(1,4.5, expression(t[2]), pos=2, col=2, cex=1)</div>
<div  class="style44" style="padding-left: 30px; text-indent:-30px"><span class="style91">#Fig. 1d</span></div>
<strong></strong>
<div class="style44" style="padding-left: 30px; text-indent:-30px">plot(0:7,0:7, type='n', axes=F, xlab='', ylab='')</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> rect(1,1,6,6)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> text(3.5,.5, expression(X[1]), pos=1, cex=1.1)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> text(.65,3.5,expression(X[2]),pos=2,cex=1.1)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> segments(2.75,1,2.75,6, col=2, lty=2, lwd=2)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> text(2.75,1, expression(t[1]), pos=1, col=2, cex=1)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> segments(1,4.5,2.75,4.5, col=2, lty=2, lwd=2)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> text(1,4.5, expression(t[2]), pos=2, col=2, cex=1)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> segments(4,1,4,6, col=2, lty=2, lwd=2)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> text(4,1, expression(t[3]), pos=1, col=2, cex=1)</div>
<div  class="style44" style="padding-left: 30px; text-indent:-30px"><span class="style91">#Fig. 1e</span></div>
<div class="style44" style="padding-left: 30px; text-indent:-30px">plot(0:7,0:7, type='n', axes=F, xlab='', ylab='')</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> rect(1,1,6,6)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> text(3.5,.5, expression(X[1]), pos=1, cex=1.1)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> text(.65,3.5, expression(X[2]), pos=2, cex=1.1)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> segments(2.75,1,2.75,6, col=2, lty=2, lwd=2)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> text(2.75,1, expression(t[1]), pos=1, col=2, cex=1)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> segments(1,4.5,2.75,4.5, col=2, lty=2, lwd=2)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> text(1,4.5, expression(t[2]), pos=2, col=2, cex=1)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> segments(4,1,4,6, col=2, lty=2, lwd=2)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> text(4,1, expression(t[3]), pos=1, col=2, cex=1)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> segments(4,2.5,6,2.5, col=2, lty=2, lwd=2)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> text(6,2.5, expression(t[4]), pos=4, col=2, cex=1)</div>
<div  class="style44" style="padding-left: 30px; text-indent:-30px"><span class="style91">#Fig. 2a</span></div>
<div class="style44" style="padding-left: 30px; text-indent:-30px">plot(0:7,0:7,type='n',axes=F, xlab='', ylab='')</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> rect(1,1,6,6)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> text(3.5,.5,expression(X[1]), pos=1, cex=1.1)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> text(.65,3.5,expression(X[2]), pos=2, cex=1.1)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> segments(2.75,1,2.75,6, col=2, lty=2, lwd=2)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> text(2.75,1,expression(t[1]), pos=1, col=2, cex=1)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> segments(1,4.5,2.75,4.5, col=2, lty=2, lwd=2)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> text(1,4.5,expression(t[2]), pos=2, col=2, cex=1)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> segments(4,1,4,6, col=2, lty=2, lwd=2)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> text(4,1, expression(t[3]), pos=1, col=2, cex=1)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> segments(4,2.5,6,2.5, col=2, lty=2, lwd=2)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> text(6,2.5, expression(t[4]), pos=4, col=2, cex=1)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px">text(1.875,2.75, expression(R[1]), cex=1.1, col=4)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> text(1.875,5.25, expression(R[2]), cex=1.1, col=4)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> text(3.375,3.625, expression(R[3]), cex=1.1, col=4)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> text(5,1.75, expression(R[4]), cex=1.1, col=4)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> text(5,4.25, expression(R[5]), cex=1.1, col=4)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px">t1&lt;-2.75</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> t2&lt;-4.5</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> t3&lt;-4</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> t4&lt;-2.5</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px">R1&lt;-function(x,y) x&gt;=1 &amp; x&lt;=t1 &amp; y&gt;=1 &amp; y&lt;=t2</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> R2&lt;-function(x,y) x&gt;=1 &amp; x&lt;=t1 &amp; y&gt;=t2 &amp; y&lt;=6</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> R3&lt;-function(x,y) x&gt;=t1 &amp; x&lt;=t3 &amp; y&gt;=1 &amp; y&lt;=6</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> R4&lt;-function(x,y) x&gt;=t3 &amp; x&lt;=6 &amp; y&gt;=1 &amp; y&lt;=t4</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> R5&lt;-function(x,y) x&gt;=t3 &amp; x&lt;=6 &amp; y&gt;=t4 &amp; y&lt;=6</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px">f1&lt;-function(x,y) R2(x,y)+ 4*R3(x,y)+ 5*R4(x,y)+ 6*R5(x,y)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px">x&lt;-seq(1,6,length=100)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> y&lt;-seq(1,6,length=100)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> g&lt;-expand.grid(x=x,y=y)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> g$z&lt;-f1(g$x,g$y)</div>
<div  class="style44" style="padding-left: 30px; text-indent:-30px"><span class="style91">#Fig. 2b</span></div>
<div class="style44" style="padding-left: 30px; text-indent:-30px">library(lattice)</div>
<div class="style44" style="padding-left: 30px; text-indent:-30px"> wireframe(z~x*y,g, xlab=expression(X[1]), ylab=expression(X[2]), zlab='Y', shade=T, light.source = c(10,10,0))</div>
<p align="center"><a href="../../index.html">Course Home Page</a> </p>
<hr align="center" width="75%">
<!--Standard footer follows -->
<p></p>
<table width="650" border="3" cellspacing="2" cellpadding="2" align=
"CENTER">
  <tr bgcolor="#CCCCCC">
    <td width="100%"><font size=-1>Jack Weiss<br>
          <i>Phone: </i>(919) 962-5930<br>
          <i>E-Mail:</i> jack_weiss@unc.edu<br>
          <i>Address: </i>Curriculum for the Environment and Ecology, Box 3275, University of North Carolina, Chapel Hill, 27599<br>
      Copyright &copy; 2012<br>
      Last Revised--April 10, 2012<br>
      URL: <a href="lecture35.htm#lecture35" target="_self">https://sakai.unc.edu/access/content/group/2842013b-58f5-4453-aa8d-3e01bacbfc3d/public/Ecol562_Spring2012/docs/lectures/lecture35.htm</a></font></td>
  </tr>
</table>
<p align="center">&nbsp;</p>
</body>
</html>
