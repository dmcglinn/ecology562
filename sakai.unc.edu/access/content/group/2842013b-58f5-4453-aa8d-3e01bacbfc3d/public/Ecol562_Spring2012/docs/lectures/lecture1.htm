<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<title>Lecture 1&mdash;Monday, January 9, 2012</title>
<link rel="stylesheet" type="text/css" href="https://sakai.unc.edu/access/content/group/2842013b-58f5-4453-aa8d-3e01bacbfc3d/public/Ecol562_Spring2012/docs/lectures/css/green.css" title="green" /> 
<link rel="stylesheet" type="text/css" href="https://sakai.unc.edu/access/content/group/2842013b-58f5-4453-aa8d-3e01bacbfc3d/public/Ecol562_Spring2012/docs/lectures/css/calendar.css" title="calendar" /> 
<link rel="alternate stylesheet" type="text/css" media="all" href="https://sakai.unc.edu/access/content/group/2842013b-58f5-4453-aa8d-3e01bacbfc3d/public/Ecol562_Spring2012/docs/lectures/css/purple.css" title="purple" /> 
<link rel="alternate stylesheet" type="text/css" media="all" href="https://sakai.unc.edu/access/content/group/2842013b-58f5-4453-aa8d-3e01bacbfc3d/public/Ecol562_Spring2012/docs/lectures/css/large.css" title="large" /> 
<link rel="alternate stylesheet" type="text/css" media="all" href="https://sakai.unc.edu/access/content/group/2842013b-58f5-4453-aa8d-3e01bacbfc3d/public/Ecol562_Spring2012/docs/lectures/css/reverse.css" title="reverse" /> 
<!-- the @import method only works from 5.0 and upwards  -->
<!-- so, using @import would "hide" the more sophisticated sheet from < 5.0 browsers -->
<!-- <style type="text/css" media="all">@import "fancy_style.css";</style> -->
<script language="JavaScript" type="text/javascript" src="https://sakai.unc.edu/access/content/group/2842013b-58f5-4453-aa8d-3e01bacbfc3d/public/Ecol562_Spring2012/docs/lectures/js/styleswitcher.js"></script> 
<style type="text/css">
<!--
div.figure {float:none;width=25%;} 
div.figure p {test-aligh: center;font-style:italic;}
div.figureL {float:left;width=50%; margin:1.5em;padding:4px 4px 4px 0px;} 
div.figureL p {test-aligh: center;font-style:italic;}
div.figureR {float:right;width=50%;margin:1.5em;padding:4px 4px 4px 0px;} 
div.figureR p {test-aligh: center;font-style:italic;}

a:link {color: #0000CC; text-decoration:none}
a:visited {color: #0000CC; text-decoration:none}
a:hover {color: green; text-decoration:underline; background:#F9EDED}
a:active {color: red; text-decoration:none}

.style1 {
	color: #CC0000;
	font-weight: bold;
}
.style3 {
	color: #CC0000;
	font-weight: bold;
}
.style4 {color: #CCCCCC}
.style7 {font-family: "Courier New", Courier, mono}
.style8 {
	font-family: Arial, Helvetica, sans-serif;
	color: #810000;
}
.style9 {
	color: #3333CC;
	font-weight: bold;
}
.styleArial {
	font-family: Arial, Helvetica, sans-serif;font-size:11.0pt;
}
.style23 {
	font-family: "Courier New", Courier, mono;
	color: #000000;
	background-color:#F0F0F0;
}
.style10 {
	font-family: "Courier New", Courier, mono;
	color: #000000;
	
}

.style24 {
	font-family: "Courier New", Courier, mono;
	color: #0000FF;
	font-size:small;
}
.style25 {
	font-family: "Courier New", Courier, mono;
	color: #0000FF;
	background-color:#FFFC9A;
}
.style15 {font-family: "Courier New", Courier, mono; color: #339933; font-weight: bold; background-color:#F0F0F0; }
.style100 {
	background-color:#FFFC9A;
}

-->
</style>
</head>

<body>
<h1 align="center"><a name="lecture1"></a>Lecture 1&mdash;Monday, January 9, 2012</h1>
<h3>Topics</h3>
<ul>
  <li><a href="lecture1.htm#simple">Review of simple linear regression</a>
    <ul>
      <li><a href="lecture1.htm#OLS">Method 1: Ordinary least squares (OLS)</a></li>
      <li><a href="lecture1.htm#method2">Method 2: Linear algebra</a></li>
      <li><a href="lecture1.htm#role">The role of statistics in regression</a></li>
    </ul>
  </li>
  <li><a href="lecture1.htm#multiple">Multiple regression</a>
    <ul>
      <li><a href="lecture1.htm#why">Why multiple regression?</a>  </li>
    </ul>
  </li>
</ul>
<h3>Terminology </h3>
<ul>
  <li><a href="lecture1.htm#confounder">confounder</a></li>
  <li><a href="lecture1.htm#interaction">interaction</a></li>
  <li><a href="lecture1.htm#OLS">ordinary least squares</a></li>
  <li><a href="lecture1.htm#simple">predictor</a></li>
  <li><a href="lecture1.htm#simple">response variable</a></li>
</ul>
<h2 align="center"><a name="simple" id="simple"></a>Review of simple linear regression</h2>

<p>In the classic	case of simple linear regression we have one continuous variable called the <span class="style8">response</span>, usually denoted <em>y</em>, and a second continuous variable called the <span class="style8">predictor</span>, usually denoted <em>x</em>. The observations consist of the ordered pairs <img src="../../images/lectures/lecture1/orderepair.gif" alt="ordered pair" width="62" height="32" align="absmiddle">, <em>i</em> = 1, 2, &hellip; , <em>n</em>. A scatter plot of <em>y</em> versus <em>x</em> suggests there may be a systematic relationship between the two variables (Fig. 1a). In the simplest case we look for a straight-line relationship between them, an equation of the form</p>
<p align="center"><img src="../../images/lectures/lecture1/regeqn.gif" width="108" height="27" alt="reg eqn"></p>
<p>where &beta;<sub>1</sub> is the slope and &beta;<sub>0</sub> is the intercept, and <img src="../../images/lectures/lecture1/yihat.gif" alt="yihat" width="20" height="27" align="absmiddle">   denotes the corresponding point on the regression line (also called the predicted value).</p>
<h3><a name="OLS"></a>Method 1: Ordinary least squares (OLS)</h3>
<p>The OLS solution to the linear regression problem is to find &beta;<sub>0</sub> and &beta;<sub>1</sub> that minimize the following quantity.</p>
<p align="center"><img src="../../images/lectures/lecture1/ols.gif" width="338" height="67"></p>
<p>The quantity being squared is referred to as the regression error  and corresponds to the  vertical deviation of an observation from the regression line (Fig. 1b). The entire quantity that is minimized is called the sum of squared errors (SSE).</p>
<div align="center">
  <table width="700" border="0" cellpadding="5">
    <tr>
      <td><div align="center">(a) <img src="../../images/lectures/lecture1/fig1a.png" alt="fig 1a" width="275" height="260" align="top"></div></td>
      <td><div align="center">(b) <img src="../../images/lectures/lecture1/fig1b.png" alt="fig 1b" width="325" height="260" align="top"></div></td>
    </tr>
    <tr>
      <td colspan="2" class="styleArial" style="padding-left: 55px; text-indent:-45px"><strong>Fig. 1</strong> (a) Scatter plot of y versus <em>x</em> suggesting a linear relationship. (b) The OLS regression line  minimizes the sum of the squared vertical deviations from the regression line, <img src="../../images/lectures/lecture1/SSE.gif" alt="SSE" width="100" height="58" align="absmiddle"></td>
    </tr>
  </table>
</div>
<p>This  problem is easily solved using calculus. To do so we obtain two derivatives, the derivative of SSE  with respect to &beta;<sub>0</sub> and the derivative of SSE with respect to &beta;<sub>1</sub>, set both derivatives equal to zero, and solve simultaneously for the parameters &beta;<sub>0</sub> and &beta;<sub>1</sub>. We write the solution as follows.</p>
<p align="center"><img src="../../images/lectures/lecture1/OLSsoln.gif" width="108" height="33"></p>
<p>where we place carets over the parameter symbols to indicate that these are the values that were estimated from the data using OLS.</p>
<p>Ordinary least squares is just one way of constructing a &quot;best-fitting&quot; line to a scatter plot of data. Other methods include the following.</p>
<ol>
  <li>Instead of minimizing the sum of the squared vertical deviations we could minimize the sum of the absolute values of the vertical deviations, <img src="../../images/lectures/lecture1/L1.gif" alt="L1 norm" width="83" height="58" align="absmiddle">. This yields the L<sub>1</sub>-norm solution to the regression problem.</li>
  <li>Rather than minimizing the sum of squared vertical distances from the line we could use squared perpendicular (shortest) distances. This yields what's called the major axis (MA) regression solution.</li>
  <li>We could use each point to construct a right triangle: one leg of the triangle is the vertical deviation of the point from the line, the second leg of the triangle is the point's horizontal deviation from the line, and the hypotenuse of the triangle consists of  the line itself. We could then minimize the sum of the areas of all the little triangles we create. This is called reduced major axis (RMA) regression.</li>
</ol>
<p>The second and third methods listed above are sometimes referred to as Type II regressions, whereas OLS is referred to as Type I regression. Historically the OLS solution has been preferred because it was considered simpler to compute and  has nice properties.</p>
<h3><a name="method2"></a>Method 2: Linear algebra</h3>
<p>The regression problem of fitting a line to a 2-dimensional scatter plot of data can also be treated as a problem in <em>n</em>-dimensional geometry. The <em>n</em> observed values of the response variable define an <em>n</em> &times; 1 vector <strong>y</strong> and the corresponding <em>n</em>  values of the predictor define an <em>n</em> &times; 1 vector <strong>x</strong>. The regression equation relates these two vectors in a matrix equation as follows.</p>
<p align="center"><img src="../../images/lectures/lecture1/system.gif" width="433" height="132" alt="system"></p>
<p>The matrix <strong>X</strong> is referred to as the design matrix of the system. Row-reducing the augmented matrix [<strong>X</strong>|<strong>y</strong>] to find a solution to this matrix equation is futile here. Linear algebra theory tells us that an <em>n</em> &times; 2 system of equations, where <em>n</em> &gt; 2, typically corresponds to an inconsistent system of equations and will not have a solution. Of course we already knew this because  we've seen that the points don't all lie on a single straight line (Fig. 1a). </p>
<p>The matrix product <strong>X&beta;</strong> can be interpreted as taking a linear combination of the columns of the matrix <strong>X</strong> using coefficients that are the components of <strong>&beta;</strong>.</p>
<p align="center"><img src="../../images/lectures/lecture1/lincombo.gif" width="502" height="132" alt="linear combination"></p>
<p>The vectors <strong>y</strong>, <strong>1</strong>, and <strong>x</strong> in the above equation can be visualized as three vectors in <em>n</em>-dimensional space (Fig. 2). The vectors <strong>1</strong> and <strong>x</strong> define a plane in <em>n</em>-dimensional space (as do any two non-collinear vectors) and by specifying values for &beta;<sub>0</sub> and &beta;<sub>1</sub> in the above linear combination we obtain different points in that plane. As Fig. 2 shows, the vector <strong>y</strong> will typically not lie in the plane defined by the vectors <strong>1</strong> and <strong>x</strong>. For that reason the matrix equation fails to have a solution. The vector <strong>y</strong> cannot be written as a linear combination of the vectors <strong>1</strong> and <strong>x</strong>.</p>
<p align="center"><img src="../../images/lectures/lecture1/projection3.png" width="500" height="300" alt="fig. 1"></p>
<p align="center" class="styleArial"><strong>Fig. 2 </strong>&nbsp;The regression problem  viewed as vector projection  in <em>n</em>-dimensional space</p>
<p>We can salvage a solution to this problem as follows. Find the vector <img src="../../images/lectures/lecture1/yhat.gif" alt="yhat" width="17" height="27" align="absmiddle"> that lies in the plane defined by the vectors <strong>1</strong> and <strong>x</strong> such that its distance from the vector <strong>y</strong> is  shortest. Geometrically we can obtain <img src="../../images/lectures/lecture1/yhat.gif" alt="yhat" width="17" height="27" align="absmiddle"> by dropping a perpendicular from <strong>y</strong> onto the plane defined by <strong>1</strong> and <strong>x</strong>. Algebraically we do this by projecting the vector <strong>y</strong> onto this plane, a feat that is accomplished by premultiplying <strong>y</strong> by an appropriate projection matrix. The components of the coefficient vector <strong>&beta;</strong> are obtained as follows.</p>
<p align="center"><img src="../../images/lectures/lecture1/betahat.gif" width="140" height="40" alt="betahat"></p>
<p>and the projected vector <img src="../../images/lectures/lecture1/yhat.gif" alt="yhat" width="17" height="27" align="absmiddle"> is given by</p>
<p align="center"><img src="../../images/lectures/lecture1/matrixsoln.gif" width="203" height="40" alt="solution"></p>
<p>where <img src="../../images/lectures/lecture1/projectionmatrix.gif" alt="projection matrix" width="115" height="40" align="absmiddle"> is the desired projection matrix. </p>
<p>Interestingly this solution obtained using projection matrices is identical to the ordinary least squares (OLS) solution. So, we get the same answer to the regression problem regardless of whether we view it as a calculus minimization problem or a vector projection problem. Either way finding the coefficients of the regression equation  is a straight-forward mathematical problem with a well-defined solution.</p>
<h3><a name="role"></a>The role of statistics in regression</h3>
<p>Everything we've done so far is just mathematics. So, where does statistics enter the picture? Mathematics gives us a line that best fits the data, where we define &quot;best&quot; in a way that suits us. If the data were obtained as a sample from a population then the  equation fit to that specific data set is  probably not that interesting  by itself. It is the corresponding regression equation in the sampled population that we truly care about. The target population may be real (as when we have a list of potential observations and we've obtained only a subset of them) or more hypothetical (e.g., the set of all possible ways we might lay out a set of quadrats in a field and then count the number of individuals of a particular species in each quadrat). </p>
<p>What we  want to be able to say is that the regression equation we obtained using our data set also represents the regression equation in the population. The concern we should have with estimating a regression equation based on a sample is that if we were to go back and collect  data a second time the sample we would obtain would be different and  produce a different regression equation. Because we know a sample-derived regression equation will vary from sample to sample we need to quantify how much it is likely to vary and include this information in our report.</p>
<p>Based on a single sample what can we say about the population that gave rise to it? Surprisingly, quite a lot. We don't actually have to go back and get additional samples.</p>
<ol>
  <li>If our sample is a random (or at least a probabilistic) sample from the population, then there is standard statistical theory that tells us how much the statistics we calculate using that sample are likely to vary if we were able to obtain multiple samples from the same population. For instance, statistical theory tells us that the sample mean based on a sample of size <em>n</em> has a variance of </li>
</ol>
<p align="center"><img src="../../images/lectures/lecture1/varofmean.gif" width="110" height="55" alt="variance of sample mean"></p>
<blockquote>
  <p>where &sigma;<sup>2</sup> can be estimated from  <em>s</em><sup>2</sup>, the variance of the sample data.</p>
</blockquote>
<ol>
  <li value=2>If we can assume that the response variable <em>y</em> is normally distributed, statistical theory tells us that the OLS solutions <img src="../../images/lectures/lecture1/beta0hat.gif" alt="beta0 hat" width="25" height="33" align="absmiddle"> and <img src="../../images/lectures/lecture1/beta1hat.gif" alt="beta1 hat" width="22" height="33" align="absmiddle"> are also normally distributed. The means of these normal distributions equal the corresponding population values and their variances can be estimated directly from the least squares fit.</li>
  <li>If the response variable <em>y</em> is not normally distributed then the least squares solutions are not necessarily normally distributed. It turns out in this case  that if we use maximum likelihood estimation instead of OLS, the regression coefficients will again be normally distributed regardless of the distribution of <em>y</em>. We'll discuss this approach to estimation beginning in week 3 of this course.</li>
</ol>
<p>The OLS solution yields the best-fitting line to a data set, but to go further and draw inferences about the population from which the data were obtained we generally need to make additional assumptions about the sample and the population. The  regression equation is then used to characterize a particular parameter in the distribution of the response variable <em>y</em>, often the mean of that distribution.</p>
<h2 align="center"> <a name="multiple"></a>Multiple regression</h2>
<p>Multiple regression differs from simple linear regression in that  we have multiple predictor variables. Mathematically multiple regression is just a trivial extension of the simple linear regression problem. The methods of solution that worked for simple linear regression also work for multiple regression. For example, if the multiple regression problem has three predictors <em>x</em>, <em>z</em>, and <em>w</em> so that the regression equation is</p>
<p align="center"><img src="../../images/lectures/lecture1/multiplereg.gif" width="213" height="27" alt="multiple regression"></p>
<p>we can find the least squares solution by constructing the sum of squares error as before</p>
<p align="center"><img src="../../images/lectures/lecture1/ols2.gif" width="330" height="58" alt="ols"></p>
<p>The only difference is that now instead of two partial derivatives there are four partial derivatives to calculate and set equal to zero. From a linear algebra perspective  the difference between simple and multiple regression is even more trivial. In multiple regression the design matrix <strong>X</strong> just has more columns in it.</p>
<p align="center"><img src="../../images/lectures/lecture1/lincombo2.gif" width="302" height="137" alt="linear algebra"></p>
<p>The solution is found in the same way as before, <img src="../../images/lectures/lecture1/betahat.gif" alt="betahat" width="140" height="40" align="absmiddle">.</p>
<p>It's worth noting that the thing that makes linear regression &quot;linear&quot; is the fact that the regression equations can be written in terms of matrix multiplication. This means that fitting curves such as polynomials to a data set is still a linear regression problem even though the curve we obtain isn't a line. Suppose we wish to fit a parabola to a scatter plot of <em>y</em> versus <em>x</em>.</p>
<p align="center"><img src="../../images/lectures/lecture1/quadratic.gif" width="163" height="30" alt="quadratic equation"></p>
<p>This is still a problem in linear regression because the regression equation can be expressed in matrix form as follows.</p>
<p align="center"><img src="../../images/lectures/lecture1/quadraticmatrix.gif" width="263" height="140" alt="quadratic matrix"></p>
<p>Having powers or functions of predictors in a regression equation poses no new problems. The equation is still linear. If the regression equation can be written as a matrix product of a data matrix times a vector of parameters, it's a linear equation. </p>
<h3><a name="why"></a>Why multiple regression?</h3>
<p>Why is  multiple regression  generally  preferable to simple linear regression?</p>
<ol>
  <li>We expect ecological and environmental processes to respond to multiple inputs. Although technically regression explores correlative relations, not causal ones, if theory suggests that there are multiple potential causative agents of a response, it  makes sense to include these different agents as predictors in a regression equation. In a formal experiment it is always a more efficient use of resources to run the experiment with multiple factors affecting the response than to run separate experiments for each factor of interest. So, here again multiple regression is the preferred method of analysis.</li>
  <li><a name="confounder"></a>When there is only a single predictor in the regression model it is possible to misinterpret that predictor's true relationship with the response. If  additional variables are added as predictors in this regression equation, we may discover that the  relationship between the response and the original targeted predictor is now quite different from what it was with just the targeted predictor alone. When this happens  the additional predictors are referred to as <span class="style8">confounding</span> variables. Controlling for confounding is one of the primary uses of multiple regression.</li>
  <li><a name="interaction"></a>Including  two or more variables  in a regression equation allows us to  investigate the phenomenon of <span class="style8">interaction</span>. If the predictors <em>x</em> and <em>z</em> interact it means that the relationship between<em> y</em> and <em>x</em> is different for different values of <em>z</em>. This may sound like confounding but it isn't. With confounding the relationship between <em>y</em> and <em>x</em> doesn't change according to the value of <em>z</em>. It stays the same for each different value of <em>z</em> but it  is different from what it was when we ignored the existence of <em>z</em> altogether. Generally speaking, if we find that the predictors <em>x</em> and <em>z</em> interact then the notion of confounding is not  applicable. Therefore the usual protocol is to first check for interaction  and then check for confounding only if interaction is absent.</li>
</ol>
<p>We'll explore the concepts of confounding and interaction further in lecture 2.</p>
<p align="center"><a href="../../index.html">Course Home Page</a> </p>
<hr align="center" width="75%">
<!--Standard footer follows -->
<p></p>
<table width="650" border="3" cellspacing="2" cellpadding="2" align=
"CENTER">
  <tr bgcolor="#CCCCCC">
    <td width="100%"><font size=-1>Jack Weiss<br>
          <i>Phone: </i>(919) 962-5930<br>
          <i>E-Mail:</i> jack_weiss@unc.edu<br>
          <i>Address: </i>Curriculum of the Environment and Ecology, Box 3275, University of North Carolina, Chapel Hill, 27599<br>
      Copyright &copy; 2012<br>
      Last Revised--Jan 11, 2012<br>
      URL: <a href="lecture1.htm#lecture1" target="_self">https://sakai.unc.edu/access/content/group/2842013b-58f5-4453-aa8d-3e01bacbfc3d/public/Ecol562_Spring2012/docs/lectures/lecture1.htm</a></font></td>
  </tr>
</table>
<p align="center">&nbsp;</p>
</body>
</html>
