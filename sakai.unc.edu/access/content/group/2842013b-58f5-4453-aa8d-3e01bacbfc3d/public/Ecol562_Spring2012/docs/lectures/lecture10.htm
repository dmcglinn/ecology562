<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<title>Lecture 10&mdash;Wednesday, February 1, 2012</title>
<link rel="stylesheet" type="text/css" href="https://sakai.unc.edu/access/content/group/2842013b-58f5-4453-aa8d-3e01bacbfc3d/public/Ecol562_Spring2012/docs/lectures/css/green.css" title="green" /> 
<link rel="stylesheet" type="text/css" href="https://sakai.unc.edu/access/content/group/2842013b-58f5-4453-aa8d-3e01bacbfc3d/public/Ecol562_Spring2012/docs/lectures/css/calendar.css" title="calendar" /> 
<link rel="alternate stylesheet" type="text/css" media="all" href="https://sakai.unc.edu/access/content/group/2842013b-58f5-4453-aa8d-3e01bacbfc3d/public/Ecol562_Spring2012/docs/lectures/css/purple.css" title="purple" /> 
<link rel="alternate stylesheet" type="text/css" media="all" href="https://sakai.unc.edu/access/content/group/2842013b-58f5-4453-aa8d-3e01bacbfc3d/public/Ecol562_Spring2012/docs/lectures/css/large.css" title="large" /> 
<link rel="alternate stylesheet" type="text/css" media="all" href="https://sakai.unc.edu/access/content/group/2842013b-58f5-4453-aa8d-3e01bacbfc3d/public/Ecol562_Spring2012/docs/lectures/css/reverse.css" title="reverse" /> 
<!-- the @import method only works from 5.0 and upwards  -->
<!-- so, using @import would "hide" the more sophisticated sheet from < 5.0 browsers -->
<!-- <style type="text/css" media="all">@import "fancy_style.css";</style> -->
<script language="JavaScript" type="text/javascript" src="https://sakai.unc.edu/access/content/group/2842013b-58f5-4453-aa8d-3e01bacbfc3d/public/Ecol562_Spring2012/docs/lectures/js/styleswitcher.js"></script> 
<style type="text/css">
<!--
div.figure {float:none;width=25%;} 
div.figure p {test-aligh: center;font-style:italic;}
div.figureL {float:left;width=50%; margin:1.5em;padding:4px 4px 4px 0px;} 
div.figureL p {test-aligh: center;font-style:italic;}
div.figureR {float:right;width=50%;margin:1.5em;padding:4px 4px 4px 0px;} 
div.figureR p {test-aligh: center;font-style:italic;}

a:link {color: #0000CC; text-decoration:none}
a:visited {color: #0000CC; text-decoration:none}
a:hover {color: green; text-decoration:underline; background:#F9EDED}
a:active {color: red; text-decoration:none}

.style1 {
	color: #CC0000;
	font-weight: bold;
}
.style3 {
	color: #CC0000;
	font-weight: bold;
}
.style4 {color: #CCCCCC}
.style7 {font-family: "Courier New", Courier, mono}
.style8 {
	font-family: Arial, Helvetica, sans-serif;
	color: #810000;
}
.style9 {
	color: #3333CC;
	font-weight: bold;
}
.styleArial {
	font-family: Arial, Helvetica, sans-serif;font-size:11.0pt;
}
.style23 {
	font-family: "Courier New", Courier, mono;
	color: #000000;
	background-color:#F0F0F0;
}
.style11 {
	font-family: "Courier New", Courier, mono;
	color: #000000;
	
}

.style22 {color: #663366; font-weight: bold; }

.style10 {
	font-family: "Courier New", Courier, mono;
	color: #000000;
	background-color:#F0F0F0;
}

.style24 {
	font-family: "Courier New", Courier, mono;
	color: #0000FF;
	font-size:small;
}
.style25 {
	font-family: "Courier New", Courier, mono;
	color: #0000FF;
	background-color:#FFFC9A;
	font-size:small;
}
.style15 {font-family: "Courier New", Courier, mono; color: #339933; font-weight: bold; background-color:#F0F0F0; }
.style100 {
	background-color:#FFFC9A;
}



.style19 {color: #339933;
	font-weight: bold;}

.style42 {color: #0000FF; font-weight: bold; font-family: "Courier New", Courier, mono;  background-color:#F0F0F0;}
.style102 {color: #CC0000;
	font-weight: bold;
}
.style12 {color: #CC0000;
	font-weight: bold;
}
.style31 {color: #336699; font-weight: bold; }
.styleArial1 {	font-family: Arial, Helvetica, sans-serif;
	font-size:11.0pt;
}
.styleArial1 {	font-family: Arial, Helvetica, sans-serif;font-size:11.0pt;
}

-->
</style>
</head>

<body>
<h1 align="center"><a name="lecture10" id="lecture10"></a>Lecture 10&mdash;Wednesday, February 1, 2012</h1>
<h2>Topics</h2>
<ul>
  <li><a href="lecture10.htm#model">AIC for model selection</a>
    <ul>
      <li><a href="lecture10.htm#kullback">Kullback-Leibler information</a></li>
      <li><a href="lecture10.htm#caveats">Caveats with using AIC for model selection</a>
        <ul>
          <li><a href="lecture10.htm#caveat1">The same data must be used in fitting each model</a></li>
          <li><a href="lecture10.htm#caveat2">The same response variable must be used in fitting each model</a></li>
        </ul>
      </li>
      <li><a href="lecture10.htm#examples">R examples</a></li>
    </ul>
  </li>
  <li><a href="lecture10.htm#probability">Probability distributions for count data: the Poisson distribution</a> 
    <ul>
      <li><a href="lecture10.htm#characteristics">Characteristics of the Poisson distribution</a></li>
      <li><a href="lecture10.htm#assumptions">Assumptions of the Poisson distribution</a></li>
      <li><a href="lecture10.htm#mass">The Poisson probability mass function</a></li>
      <li><a href="lecture10.htm#motivation">Motivation for the Poisson distribution</a></li>
    </ul>
  </li>
  <li><a href="lecture10.htm#cited">Cited references</a></li>
</ul>
<h2>R functions and commands demonstrated</h2>
<ul>
  <li><a href="lecture10.htm#AIC">AIC</a> returns the AIC of a model.</li>
  <li><a href="lecture10.htm#isna">is.na</a> is a Boolean function that determines if a value is missing, NA, or not.</li>
  <li><a href="lecture10.htm#isna">!</a> is the logical not operator of R.</li>
</ul>
<h2><a name="model" id="model"></a>AIC for model selection</h2>
<p>Maximizing the likelihood can be used as a criterion both for parameter estimation and model selection. A better model is one that yields a larger  value of the likelihood. Because likelihood can't decrease as variables are added to a model, using likelihood alone for model selection will always lead one to choose the most complicated model.  Hence we need to include a penalty for model complexity. Last time we introduced the formulas for AIC and its small sample replacement AIC<sub>c</sub>, two examples of penalized log-likelihood criteria for model selection.</p>
<p align="center"><img src="../../images/lectures/lecture10/informationcriteria.gif" width="265" height="82" alt="AIC"></p>
<p>Here <em>k</em> is the number of model parameters and <em>n</em> is the sample size.   AIC is an acronym for Akaike Information Criterion.</p>
<p> The model's log-likelihood appears in the first term of   AIC, &ndash;2log <em>L</em>, but with the opposite sign. Thus  models with larger log-likelihoods will have lower values of AIC. The second term in AIC is positive and  acts to increase AIC. It plays the role of a penalty term becoming larger as the model gets more complex. But AIC is more than just penalized log-likelihood. It also has a strong theoretical connection to information theory. We briefly discuss this connection next. Further details can be found in Burnham and Anderson (2002).</p>
<h3><a name="kullback"></a>Kullback-Leibler information</h3>
<p name="KL">The <span class="style31">Kullback-Leibler (K-L) information</span> between models <i>f</i> and <i>g </i>is defined for continuous distributions as follows.</p>
<p align="center"><img src="../../images/lectures/lecture10/KLinfo.gif" width="267" height="65" alt="KL info"></p>
<p>Here <i>f</i> and <i>g</i> are probability distributions and <strong>&theta; </strong>denotes the various parameters used in the specification of <i>g</i>. <i>I</i>(<i>f,</i> <i>g</i>) has two equivalent interpretations.</p>
<ul>
  <li><i>I</i>(<i>f,</i> <i>g</i>) represents the distance from model <i>g</i> to model <i>f</i>. </li>
  <li><i>I</i>(<i>f,</i> <i>g</i>) is the information lost when using <i>g</i> to approximate <i>f</i>. </li>
</ul>
<p>Typically, <i>g</i> is taken to be an approximating model  while <i>f</i> is taken to be the true state of nature.  As a measure of distance, <i>I</i>(<i>f</i>, <i>g</i>) is a strange beast. Because of the asymmetry in the way <i>f</i> and <i>g</i> are treated in the integral, <i>I</i>(<i>f</i>, <i>g</i>) &ne; <i>I</i>(<i>g</i>, <i>f</i>). </p>
<p>Now suppose we have a second approximating model <em>h</em> that depends on a different parameter set <strong>&psi;</strong>. The K-L information between the true state of nature <em>f</em> and model <em>h</em> is defined as follows.</p>
<p align="center"><img src="../../images/lectures/lecture10/KLinfo2.gif" width="270" height="65" alt="KL info"></p>
<p>If it turns out that <em>I</em>(<em>f</em>, <em>h</em>) &lt; <em>I</em>(<em>f</em>, <em>g</em>) then we should prefer model <em>h</em> to model <em>g</em> because it is closer to the true state of nature <em>f</em>. Of course all of this ignores an obvious problem. We don't know the true state of nature <em>f</em> and therefore we can't calculate <em>I</em>(<em>f</em>, <em>g</em>) or <em>I</em>(<em>f</em>, <em>h</em>). Fortunately we don't need to know the true state of nature <em>f</em> for these ideas to be useful.</p>
<p>In Fig. 1   models <em>f</em>, <em>g</em>, and <em>h</em> are positioned on their Kullback-Leibler scale so that the vertical distance between  <em>f</em> and <em>g</em> is their K-L distance, and the vertical distance between <em>f</em> and <em>h</em> is their K-L distance. Of course we can never actually locate <em>f</em> in this picture (because <em>f</em> is unknown to us), but suppose it is possible to approximate the vertical positions (<em>y</em>-coordinates) of <em>g</em> and <em>h</em> relative to the <em>y</em> = 0 axis. As Fig. 1 shows, the model with the smaller <em>y</em>-coordinate will necessarily be closer to <em>f</em>. Thus we can use the <em>y</em>-coordinates of the models as a way to choose between them. We don't need to know the location of <em>f</em> in Fig. 1, just the relative positions of <em>g</em> and <em>h</em> with respect to each other. </p>
<table width="550" border="0" align="center" cellpadding="5">
  <tr>
    <td><div align="center"><img src="../../images/lectures/lecture10/fig3.png" width="464" height="248" alt="fig 3"></div></td>
  </tr>
  <tr>
    <td class="styleArial1" style="padding-left: 45px; text-indent:-45px"><strong>Fig. 1</strong> &nbsp;The connection between AIC and K-L information. While the magnitude of K-L information is interpretable,  the magnitude of AIC tells us nothing about the absolute quality of a model. AIC only provides relative information. (<a href="https://sakai.unc.edu/access/content/group/2842013b-58f5-4453-aa8d-3e01bacbfc3d/public/Ecol562_Spring2012/notes/lecture10fig1&#32;Rcode.txt">R code</a>)</td>
  </tr>
</table>
<p>Akaike (1971) showed that AIC  provides an approximate estimate of the relative position of models with respect to their K-L information. Models that have smaller values of AIC are better models than comparable models with larger values of AIC.  Because AIC is only a relative measure its numerical value is meaningless. In Fig. 1 we see that AIC is measured with respect to numerical zero rather than  to the absolute zero, which in the figure corresponds to the true state of nature. Because its reference point is arbitrary, the absolute magnitude of AIC is not interpretable. </p>
<p>AIC is typically positive, although it can be negative. If the likelihood is constructed from a discrete probability model, the individual terms in the log-likelihood will be log probabilities. These are negative because probabilities are numbers between 0 and 1. AIC multiplies these terms by &ndash;2 and hence AIC will be positive for discrete probability models. In continuous probability models  the terms of the log-likelihood are log densities.  Because a probability density can exceed one AIC can be negative or positive.</p>
<h3><a name="caveats"></a>Caveats with using AIC for model selection</h3>
<p><strong><a name="caveat1"></a>Caveat 1</strong>. AIC can only be used to compare models that are fit using exactly the same set of observations. This fact can cause problems when fitting regression models to different sets of variables if there are missing values in the data. Consider the following data set consisting of five observations and three variables where one of the variables has a missing value (NA) for  observation 3.</p>
<table width=250 border=1 align="center" cellpadding=2 cellspacing=2 frame=box rules=groups>  
  <colgroup>
  </colgroup>
<thead>
  <tr bgcolor="#F1D2D8">
    <td valign="top" align="center">Observation</td>

    <td valign="top" align="center"><em>y</em></td>
    <td valign="top" align="center"><em>x</em><sub>1</sub></td>
    <td valign="top" align="center"><em>x</em><sub>2</sub></td>
  </tr>  </thead> <tbody>
  <tr>
    <td><div align="center">1</div></td>
    <td><div align="center">&ndash;</div></td>
    <td><div align="center">&ndash;</div></td>
    <td><div align="center">&ndash;</div></td>
  </tr>
  <tr>
    <td><div align="center">2</div></td>
    <td><div align="center">&ndash;</div></td>
    <td><div align="center">&ndash;</div></td>
    <td><div align="center">&ndash;</div></td>
  </tr>
  <tr>
    <td><div align="center">3</div></td>
    <td><div align="center">&ndash;</div></td>
    <td><div align="center">&ndash;</div></td>
    <td><div align="center">NA</div></td>
  </tr>
  <tr>
    <td><div align="center">4</div></td>
    <td><div align="center">&ndash;</div></td>
    <td><div align="center">&ndash;</div></td>
    <td><div align="center">&ndash;</div></td>
  </tr>
  <tr>
    <td><div align="center">5</div></td>
    <td><div align="center">&ndash;</div></td>
    <td><div align="center">&ndash;</div></td>
    <td><div align="center">&ndash;</div></td>
  </tr>
</table>
<p>Most regression packages carry out  case-wise deletion so that observations with missing values in any of the variables that are used in  the model are deleted automatically. Thus in fitting the following three models to these data, the number of available observations will vary.</p>
<table width=250 border=1 align="center" cellpadding=2 cellspacing=2 frame=box rules=groups>
  <colgroup>
  </colgroup>
  <thead>
    <tr bgcolor="#F1D2D8">
      <td><div align="center">Model</div></td>
      <td><div align="center">Formula</div></td>
      <td><div align="center"><em>n</em></div></td>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><div align="center">1</div></td>
      <td><div align="center"><em>y</em> ~ <em>x</em><sub>1</sub></div></td>
      <td><div align="center">5</div></td>
    </tr>
    <tr>
      <td><div align="center">2</div></td>
      <td><div align="center"><em>y</em> ~ <em>x</em><sub>2</sub></div></td>
      <td><div align="center">4</div></td>
    </tr>
    <tr>
      <td><div align="center">3</div></td>
      <td><div align="center"><em>y</em> ~ <em>x</em><sub>1</sub> + <em>x</em><sub>2</sub></div></td>
      <td><div align="center">4</div></td>
    </tr>
</table>
<p>Models 2 and 3 being fit to the same four observations are comparable using AIC, but neither of these models can be compared to the AIC of Model 1 which is based on one additional observation. The solution is to delete observation 3 and fit all three models using only the four complete observations 1, 2, 4, and 5. If model 1 ranks best among these three models then  model 1 can be refit with observation 3  included.</p>
<p>Because many regression functions do not explicitly warn you that observations have been deleted you need to check this possibility before you begin fitting models. In the <span class="style8">rikz</span> data set  we were fitting models to richness that involved the predictors NAP and week.</p>
<div class="style23" style="padding-left: 30px; text-indent:-30px">rikz &lt;- read.table('ecol 562/RIKZ.txt', header=TRUE)</div>
<div class="style23" style="padding-left: 30px; text-indent:-30px"> rikz$richness &lt;- apply(rikz[,2:76], 1, function(x) sum(x&gt;0))</div>

<p><a name="isna"></a>To check for missing values we can use the <span class="style1">apply</span> function along with <span class="style1">is.na</span> to count  the number of missing values in the variables of interest for each observation. The exclamation point, <span class="style1">!</span>, is the &quot;not&quot; symbol in R, so <span class="style11">!is.na(x)</span> checks whether a variable is not missing (evaluates to TRUE) for a given observation. We then sum this to obtain the number of non-missing values for that observation.</p>
<div class="style23" style="padding-left: 30px; text-indent:-30px">num.not.missing <- apply(rikz[,c("richness","NAP","week")], 1, function(x) sum(!is.na(x)))</div>
<div class="style23" style="padding-left: 30px; text-indent:-30px"> num.not.missing</div>
<span class="style24">  &nbsp;[1] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3</span>
<div class="style23" style="padding-left: 30px; text-indent:-30px"> table(num.not.missing)</div>
<span class="style24">  num.not.missing<br>
  &nbsp;3 <br>
45</span>
<p>We see that all 45 observations have non-missing values for all three of the variables. Consequently any model we fit involving one or more of these variables can be compared using AIC.</p>
<p><strong><a name="caveat2"></a>Caveat 2</strong>. The response variable must be the same in each model being compared. Monotonic transformations alter the probability densities at individual points in order that  the probabilities over  intervals remain the same.  The likelihood on the other hand only includes the densities at individual points. This means that the log-likelihoods and AICs of models with a transformed response, e.g. log <em>y</em>, cannot be directly compared to the log-likelihoods and AICs of models that are fit to the raw response, <em>y</em>.  Fortunately there are  ways around this problem. </p>
<ol>
  <li>For both discrete and continuous response variables we can re-express the log-likelihood in terms of the raw response by using the change of variables formula for integration. Suppose we log-transform a response variable <em>y</em> and then make the assumption that the log-transformed response is normally distributed.</li>
</ol>
<p align="center"><img src="../../images/lectures/lecture10/logy.gif" width="215" height="37" alt="log y"></p>
<blockquote>
  <p>The likelihood for the transformed model is</p>
</blockquote>
<p align="center"><img src="../../images/lectures/lecture10/likelihood.gif" width="277" height="57" alt="likelihood"></p>
<blockquote>
  <p>To  calculate probabilities we have to integrate the density.</p>
</blockquote>
<p align="center"><img src="../../images/lectures/lecture10/probability.gif" width="357" height="47" alt="probability"></p>
<blockquote>
  <p>If you make  the substitution <img src="../../images/lectures/lecture10/substitution.gif" alt="substitution" width="155" height="33" align="absmiddle"> the integral becomes the following.</p>
</blockquote>
<p align="center"><img src="../../images/lectures/lecture10/transformation.gif" width="400" height="93" alt="substitution"></p>
<blockquote>
  <p>The integrand is now a density on the  scale of the response and can be used to construct a likelihood on the scale of the response.</p>
</blockquote>
<p align="center"><img src="../../images/lectures/lecture10/likelihood2.gif" width="327" height="58" alt="likelihood"></p>
<blockquote>
  <p>This likelihood is now comparable to other likelihoods that model the raw response <em>y</em>. If <em>y</em> is discrete then each term in this expression can be viewed as the midpoint approximation to the area under the curve over the interval <img src="../../images/lectures/lecture10/yi.gif" alt="interval" width="148" height="32" align="absmiddle">  so that the likelihood can be interpreted as a probability (<a href="lecture8.htm#comparing">lecture 8</a>).</p>
</blockquote>
<ol start="2">
  <li>When the   response variable being transformed is discrete  another option is available. Because the likelihood is a probability  calculate the probability of each observation on the transformed scale over the appropriate interval. In this case the likelihood becomes</li>
</ol>
<p align="center"><img src="../../images/lectures/lecture10/likelihood3.gif" width="618" height="57" alt="likelihood"></p>
<blockquote>
  <p>This approach is preferable to using the midpoint approximation particularly if there are any observed zeros that make it necessary to add a constant to the response variable before carrying out the transformation (such as  log). In this case the above formula has to be modified slightly because the zero values are boundary values and therefore only contribute the first term in the  difference to the likelihood.</p>
</blockquote>
<h3><a name="examples"></a>R examples</h3>
<p>I fit the three normal and three Poisson models to the richness variable in the <span class="style8">rikz</span> data set that we've considered previously.</p>
<div class="style23" style="padding-left: 30px; text-indent:-30px"> mod1 &lt;- lm(richness~NAP, data=rikz)</div>
<div class="style23" style="padding-left: 30px; text-indent:-30px"> mod2 &lt;- lm(richness~NAP+factor(week), data=rikz)</div>
<div class="style23" style="padding-left: 30px; text-indent:-30px"> mod3 &lt;- lm(richness~NAP*factor(week), data=rikz)</div>
<div class="style23" style="padding-left: 30px; text-indent:-30px"> mod1p &lt;- glm(richness~NAP, data=rikz, family=poisson)</div>
 <div class="style23" style="padding-left: 30px; text-indent:-30px"> mod2p &lt;- glm(richness~NAP+factor(week), data=rikz, family=poisson)</div>
<div class="style23" style="padding-left: 30px; text-indent:-30px"> mod3p &lt;- glm(richness~NAP*factor(week), data=rikz, family=poisson)</div>
<p><a name="AIC"></a>The <span class="style1">AIC</span> function calculates the AIC of each of these models.</p>
<div class="style15" style="padding-left: 30px; text-indent:-30px"> #compare AIC of the six models</div>
<div class="style23" style="padding-left: 30px; text-indent:-30px"> AIC(mod1, mod2, mod3, mod1p, mod2p, mod3p)</div>
<span class="style24">   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; df&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; AIC<br>
  mod1&nbsp;&nbsp; 3 259.9535<br>
  mod2&nbsp;&nbsp; 6 232.9032<br>
  mod3&nbsp;&nbsp; 9 217.2449<br>
  mod1p&nbsp; 2 259.1807<br>
  mod2p&nbsp; 5 205.4644<br>
mod3p&nbsp; 8 189.0687</span>
<p>So based on AIC we should prefer the interaction model in which  richness is assumed to have a Poisson distribution. When using AIC to compare models one should report the log-likelihood, number of estimated parameters, and AIC of each model separately.</p>
<div class="style15" style="padding-left: 30px; text-indent:-30px"> #add log-likelihoods to the table</div>
<div class="style23" style="padding-left: 30px; text-indent:-30px"> data.frame(AIC(mod1, mod2 ,mod3, mod1p, mod2p, mod3p), LL=sapply(list(mod1, mod2, mod3, mod1p, mod2p, mod3p), logLik))</div>
<span class="style24">   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; df&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; AIC&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; LL<br>
  mod1&nbsp;&nbsp; 3 259.9535 -126.97674<br>
  mod2&nbsp;&nbsp; 6 232.9032 -110.45160<br>
  mod3&nbsp;&nbsp; 9 217.2449&nbsp; -99.62243<br>
  mod1p&nbsp; 2 259.1807 -127.59036<br>
  mod2p&nbsp; 5 205.4644&nbsp; -97.73222<br>
mod3p&nbsp; 8 189.0687&nbsp; -86.53437</span>
<p>Log-transforming count data and assuming the result has a normal distribution has historically been the recommended way to analyze count data particularly if the count distribution is highly skewed. Because there are zero values of richness in the <span class="style8">rikz</span> data set, the log transformation  is undefined.</p>
<div class="style15" style="padding-left: 30px; text-indent:-30px"> #log-transform richness</div>
<div class="style23" style="padding-left: 30px; text-indent:-30px">mod1.log &lt;- lm(log(richness)~NAP, data=rikz)</div>
<span class="style24">   Error in lm.fit(x, y, offset = offset, singular.ok = singular.ok, ...) : <br>
&nbsp; NA/NaN/Inf in foreign function call (arg 4)</span>
<p>The usual &quot;fix&quot; is to add a small constant, 0.5 or 1, to all the observations in the data set before carrying out the transformation. The choice of constant can make a difference in the results.</p>
<div class="style15" style="padding-left: 30px; text-indent:-30px"> #need to deal with zeros in the data</div>
<div class="style23" style="padding-left: 30px; text-indent:-30px"> mod1.log &lt;- lm(log(richness+.5)~NAP, data=rikz)</div>
<div class="style15" style="padding-left: 30px; text-indent:-30px">#log-likelihood and AIC are not comparable to those of the previous models</div>
<div class="style15" style="padding-left: 30px; text-indent:-30px"> #we need to make an adjustment to the log-likelihood</div>
<div class="style23" style="padding-left: 30px; text-indent:-30px">logLik(mod1.log)</div>
 <span class="style24">  'log Lik.' -42.19844 (df=3)</span>
<div class="style23" style="padding-left: 30px; text-indent:-30px">AIC(mod1.log)</div>
<span class="style24"> [1] 90.39687</span>
<p>The reported log-likelihood and AIC would seem to suggest that the log-transformed model handily beats all the models fit thus far. This conclusion is not necessarily correct because the reported log-likelihood is not comparable to the log-likelihoods obtained from fitting models to the raw response. We first need to make one of the  adjustments to the likelihood  described in the <a href="lecture10.htm#caveat2">previous section</a>. Using the second method the corrected log-likelihood for the log-transformed model fit above is &ndash;109.0533 and the AIC is 224.1067. We'll see how to carry out this adjustment for a different data set in the next lecture.</p>
<h2 align="center"><a name="probability"></a>Probability distributions for count data: the Poisson distribution</h2>
<h3><strong><a name="characteristics"></a>Characteristics of the Poisson distribution</strong></h3>
<p>A Poisson random variable is discrete and the Poisson distribution is a standard probability model for count data.The Poisson distribution is bounded  below by 0, but is theoretically unbounded above. This distinguishes it from another  distribution, the binomial distribution, which is bounded on both sides. The Poisson distribution is a one-parameter distribution.  The parameter is usually denoted with the symbol &lambda; and is  called the rate of the process.</p>
<p>The mean of the Poisson distribution is equal to the rate, &lambda;. The variance of the Poisson distribution is also equal to &lambda;. Thus in the Poisson distribution the variance is equal to the mean. So  if <img src="../../images/lectures/lecture10/Poisdist.gif" alt="Poisson" width="130" height="30" align="absmiddle"> then </p>
<blockquote>
  <p align="center">Mean: <img src="../../images/lectures/lecture10/Poismean.gif" width="83" height="30" align="absmiddle"></p>
  <p align="center">Variance: <img src="../../images/lectures/lecture10/Poisvar.gif" width="100" height="30" align="absmiddle"></p>
</blockquote>
<p>So in  the Poisson distribution the variance is a function of the mean, i.e., <img src="../../images/lectures/lecture10/poisson&#32;variance.gif" alt="variance" width="170" height="30" align="absmiddle">.  When  the mean gets larger, the variance gets larger at exactly the same rate. Contrast this with the normal distribution where the variance is constant and is independent of the mean.</p>
<h3><a name="assumptions"></a>Assumptions of the Poisson distribution </h3>
<p>The Poisson distribution can be derived from three basic assumptions about the underlying process that is generating the data.</p>
<ol>
  <li><span class="style31" name="homogeneity">Homogeneity</span> assumption: Events occur at a constant rate &lambda; such that on average for any length of time <em>t</em> we would expect to see <em> &lambda;t </em> events. If the events are occurring over space then for an area of size <em>A</em> we would expect to see <em>&lambda;A</em> events.</li>
  <li><span class="style31">Independence</span> assumption: For any two non-overlapping intervals (or areas) the number of observed events is independent. </li>
  <li>If the interval is very small, then the probability of observing two or more events in that interval is essentially zero.</li>
</ol>
<h3><strong><a name="mass"></a>The Poisson probability mass function</strong></h3>
<p>Let  <img src="../../images/lectures/lecture10/Poisson.gif" alt="Poisson" width="137" height="30" align="absmiddle"> where <em>N<sub>t</sub></em> is the number of events occurring in a time interval of length <em>t</em>, then the probability of observing <em>k </em>events in that interval is </p>
<p align="center"><img src="../../images/lectures/lecture10/Poisson&#32;mass.gif" width="182" height="58" alt="Poisson mass"></p>
<p>The Poisson distribution can be applied to both time and space. In a Poisson model of two-dimensional space, events occur again at a constant rate such that the number of events observed in an area A is expected on average to be &lambda;A. In this case the probability mass function is</p>
<p align="center"><img src="../../images/lectures/lecture10/Poisson&#32;mass&#32;area.gif" width="197" height="58" alt="Poisson area"></p>
<p> For spatial distributions the Poisson distribution plays a role in defining what&rsquo;s called <span class="style31">CSR&mdash;complete spatial randomness</span>.
</p>
<ul>
  <li>If we imagine moving a window of fixed size over a landscape then due to the homogeneity assumption, no matter where we move the window the picture should look essentially the same. We will on average see the same number of events in each window. This rules out clumping. If the distribution were aggregated then some snapshots from our moving window would show many events, while others would show none. Note: in a clumped distribution the variance will be greater than the mean. </li>
  <li>Due to independence the spatial distribution under CSR will not appear very regular. For a regular equally-spaced distribution to occur nearby events would have to interfere with each other to cause the regular spacing. The independence of events in non-overlapping regions means that interference of this sort is not possible. In a regular distribution the variance is smaller than the mean. <br>
  </li>
</ul>
<p>Sometimes we fix the time interval or the area for all observations. If all the quadrats are the same size then we don't need to know what <em>A</em> is. In such cases we suppress <em>t</em> and <em>A</em> in our formula for the probability mass function and write instead</p>
<p align="center"><img src="../../images/lectures/lecture10/Poisson&#32;mass&#32;general.gif" width="150" height="55" alt="Poisson mass"></p>
<p align="left">In R the above probability would be calculated as <span class="style11">dpois(k, &lambda;)</span>.</p>
<h3><a name="motivation"></a>Motivation for the Poisson distribution</h3>
<p>Unlike some probability distributions that are used largely because they resemble distributions seen in nature, but otherwise have no particular theoretical motivation, the use of the Poisson distribution can be motivated by theory. The Poisson distribution arises in practice in two distinct ways.
</p>
<ol>
  <li>Using the three <a href="lecture10.htm#assumptions">assumptions</a> listed above one can derive the formula for the probability mass function directly from first principles. The process involves setting up differential equations that describe the probability of seeing a new event in the next time interval.</li>
  <li>Another way the Poisson probability model arises in practice is as an approximation to the binomial distribution. Suppose we have binomial data in which <em>p</em>, the probability of success is very small, and <em>n</em>, the number of trials, is very large. In this case the Poisson distribution is a good approximation to the binomial where &lambda; = <em>np</em>. Formally it can be shown that if you start with the probability mass function for a binomial distribution and let <em>n</em> &rarr; &infin; and <em>p</em> &rarr; 0 in such a way that <em>np</em> remains constant, you obtain the probability mass function of the Poisson distribution.</li>
</ol>
<h2><b><a name="cited"></a>Cited references</b></h2>
<ul>
  <li>Akaike, Hirotugu. 1973. Information theory and an extension of the maximum likelihood principle. <em>Proceedings of the 2nd International Symposium on Information Theory</em> (edited by B. N. Petrov and F. Csaki). Akademiai Kiado, Budapest. (Reproduced in S. Kotz and N. L. Johnson (editors), 1992, <em>Breakthroughs in Statistics</em>, New York: Springer-Verlag, pp. 610&ndash;624.)</li>
  <li>Burnham, K. P. and D. R. Anderson. 2002. <i>Model Selection and Multimodel Inference</i>. Springer-Verlag: New York.</li>
</ul>
<p align="center"><a href="../../index.html">Course Home Page</a> </p>
<hr align="center" width="75%">
<!--Standard footer follows -->
<p></p>
<table width="650" border="3" cellspacing="2" cellpadding="2" align=
"CENTER">
  <tr bgcolor="#CCCCCC">
    <td width="100%"><font size=-1>Jack Weiss<br>
          <i>Phone: </i>(919) 962-5930<br>
          <i>E-Mail:</i> jack_weiss@unc.edu<br>
          <i>Address: </i>Curriculum for the Environment and Ecology, Box 3275, University of North Carolina, Chapel Hill, 27599<br>
      Copyright &copy; 2012<br>
      Last Revised--February 5, 2012<br>
      URL: <a href="lecture10.htm#lecture10" target="_self">https://sakai.unc.edu/access/content/group/2842013b-58f5-4453-aa8d-3e01bacbfc3d/public/Ecol562_Spring2012/docs/lectures/lecture10.htm</a></font></td>
  </tr>
</table>
<p align="center">&nbsp;</p>
</body>
</html>
