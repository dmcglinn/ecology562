<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<title>Lecture 9&mdash;Monday, January 30, 2012</title>
<link rel="stylesheet" type="text/css" href="https://sakai.unc.edu/access/content/group/2842013b-58f5-4453-aa8d-3e01bacbfc3d/public/Ecol562_Spring2012/docs/lectures/css/green.css" title="green" /> 
<link rel="stylesheet" type="text/css" href="https://sakai.unc.edu/access/content/group/2842013b-58f5-4453-aa8d-3e01bacbfc3d/public/Ecol562_Spring2012/docs/lectures/css/calendar.css" title="calendar" /> 
<link rel="alternate stylesheet" type="text/css" media="all" href="https://sakai.unc.edu/access/content/group/2842013b-58f5-4453-aa8d-3e01bacbfc3d/public/Ecol562_Spring2012/docs/lectures/css/purple.css" title="purple" /> 
<link rel="alternate stylesheet" type="text/css" media="all" href="https://sakai.unc.edu/access/content/group/2842013b-58f5-4453-aa8d-3e01bacbfc3d/public/Ecol562_Spring2012/docs/lectures/css/large.css" title="large" /> 
<link rel="alternate stylesheet" type="text/css" media="all" href="https://sakai.unc.edu/access/content/group/2842013b-58f5-4453-aa8d-3e01bacbfc3d/public/Ecol562_Spring2012/docs/lectures/css/reverse.css" title="reverse" /> 
<!-- the @import method only works from 5.0 and upwards  -->
<!-- so, using @import would "hide" the more sophisticated sheet from < 5.0 browsers -->
<!-- <style type="text/css" media="all">@import "fancy_style.css";</style> -->
<script language="JavaScript" type="text/javascript" src="https://sakai.unc.edu/access/content/group/2842013b-58f5-4453-aa8d-3e01bacbfc3d/public/Ecol562_Spring2012/docs/lectures/js/styleswitcher.js"></script> 
<style type="text/css">
<!--
div.figure {float:none;width=25%;} 
div.figure p {test-aligh: center;font-style:italic;}
div.figureL {float:left;width=50%; margin:1.5em;padding:4px 4px 4px 0px;} 
div.figureL p {test-aligh: center;font-style:italic;}
div.figureR {float:right;width=50%;margin:1.5em;padding:4px 4px 4px 0px;} 
div.figureR p {test-aligh: center;font-style:italic;}

a:link {color: #0000CC; text-decoration:none}
a:visited {color: #0000CC; text-decoration:none}
a:hover {color: green; text-decoration:underline; background:#F9EDED}
a:active {color: red; text-decoration:none}

.style1 {
	color: #CC0000;
	font-weight: bold;
}
.style3 {
	color: #CC0000;
	font-weight: bold;
}
.style4 {color: #CCCCCC}
.style7 {font-family: "Courier New", Courier, mono}
.style8 {
	font-family: Arial, Helvetica, sans-serif;
	color: #810000;
}
.style9 {
	color: #3333CC;
	font-weight: bold;
}
.styleArial {
	font-family: Arial, Helvetica, sans-serif;font-size:11.0pt;
}
.style23 {
	font-family: "Courier New", Courier, mono;
	color: #000000;
	background-color:#F0F0F0;
}
.style11 {
	font-family: "Courier New", Courier, mono;
	color: #000000;
	
}

.style22 {color: #663366; font-weight: bold; }

.style10 {
	font-family: "Courier New", Courier, mono;
	color: #000000;
	background-color:#F0F0F0;
}

.style24 {
	font-family: "Courier New", Courier, mono;
	color: #0000FF;
	font-size:small;
}
.style25 {
	font-family: "Courier New", Courier, mono;
	color: #0000FF;
	background-color:#FFFC9A;
	font-size:small;
}
.style15 {font-family: "Courier New", Courier, mono; color: #339933; font-weight: bold; background-color:#F0F0F0; }
.style100 {
	background-color:#FFFC9A;
}



.style19 {color: #339933;
	font-weight: bold;}

.style42 {color: #0000FF; font-weight: bold; font-family: "Courier New", Courier, mono;  background-color:#F0F0F0;}
.style102 {color: #CC0000;
	font-weight: bold;
}
.style12 {color: #CC0000;
	font-weight: bold;
}

-->
</style>
</head>

<body>
<h1 align="center"><a name="lecture9" id="lecture9"></a>Lecture 9&mdash;Monday, January 30, 2012</h1>
<h2>Topics</h2>
<ul>
  <li><a href="lecture9.htm#maximum">Maximum likelihood estimates</a>
    <ul>
      <li><a href="lecture9.htm#why">Why are MLEs attractive?</a></li>
      <li><a href="lecture9.htm#problems">Some problems with MLEs</a></li>
      <li><a href="lecture9.htm#OLS">Maximum likelihood estimation versus ordinary least squares</a></li>
    </ul>
  </li>
  <li><a href="lecture9.htm#significance">Significance testing and maximum likelihood estimation</a>
    <ul>
      <li><a href="lecture9.htm#likelihood">Likelihood ratio tests</a></li>
      <li><a href="lecture9.htm#wald">Wald tests</a></li>
      <li><a href="lecture9.htm#waldci">Wald confidence intervals</a></li>
      <li><a href="lecture9.htm#lrci">Likelihood-based confidence intervals</a></li>
    </ul>
  </li>
  <li><a href="lecture9.htm#model">Model selection criteria</a></li>
</ul>
<h2>R functions and commands demonstrated</h2>
<ul>
  <li><a href="lecture9.htm#attr">attr</a> can be used to extract attributes from R objects</li>
  <li><a href="lecture9.htm#logLik">logLik</a> extracts the log-likelihood from a model object</li>
  <li><a href="lecture9.htm#pchisq">pchisq</a> is the cumulative distribution function of a chi-squared distribution</li>
  <li><a href="lecture9.htm#qchisq">qchisq</a> is the quantile function of a chi-squared distribution</li>
</ul>
<h2><a name="maximum" id="maximum"></a>Maximum likelihood estimates (MLEs)</h2>
<p>We begin by considering some of the more theoretical aspects of maximum likelihood estimation. Because MLEs are calculated from a sample,  the value obtained will vary from sample to sample. Hence MLEs have a sampling distribution and we can talk about the characteristics of that distribution and using it as a basis for constructing confidence intervals.</p>
<h3><a name="why"></a>Why are MLEs attractive?</h3>
<ol>
  <li>They are intuitive. The notion of selecting values for the parameters in order to make the data most probable under a given model is appealing.</li>
  <li>MLEs have a number of nice statistical properties. Most of these are rather technical.</li>
  <li> The sampling distribution of MLEs is asymptotically normal with known variance. Thus if the sample is  large enough    statistical tests can be carried out and  confidence intervals constructed by appealing to normality. </li>
  <li>The log-likelihood has a connection to information theory that yields a method for comparing non-nested models using the AIC statistic.</li>
</ol>
<h3><a name="problems"></a>Some problems with MLEs</h3>
<ol>
  <li>There are no concrete rules for deciding when a sample is big enough for the assumption of normality to hold. Thus statistical tests carried out using maximum likelihood estimates can be suspect.</li>
  <li>MLEs can be biased with small samples.</li>
</ol>
<p>The term bias has a very specific meaning to a statistician. An estimate <img src="../../images/lectures/lecture9/thetahat.gif" alt="theta hat" width="17" height="28" align="absbottom"> is unbiased for a parameter &theta; if the mean of the sampling distribution of <img src="../../images/lectures/lecture9/thetahat.gif" alt="theta hat" width="17" height="28" align="absbottom"> is equal to &theta;. A statistician writes this as<img src="../../images/lectures/lecture9/unbiased.gif" alt="unbiased" width="75" height="32" align="absmiddle"> , where <em>E</em> is the expectation operator and is defined as &quot;take the mean&quot;. An unbiased estimator has a sampling distribution that is centered over the true population value. Fig. 1 shows the sampling distributions of two estimators, <img src="../../images/lectures/lecture9/theta1hat.gif" alt="theta1 hat" width="20" height="32" align="absmiddle"> and <img src="../../images/lectures/lecture9/theta2hat.gif" alt="theta2 hat" width="22" height="32" align="absmiddle">. Estimator <img src="../../images/lectures/lecture9/theta1hat.gif" alt="theta1 hat" width="20" height="32" align="absmiddle"> is unbiased. The mean of its distribution is equal to the population value &theta;. The estimator <img src="../../images/lectures/lecture9/theta2hat.gif" alt="theta2 hat" width="22" height="32" align="absmiddle"> is biased. It's distribution is centered on a value less than &theta; and hence on average will underestimate the population value &theta;.</p>
<p align="center"><img src="../../images/lectures/lecture9/bias.png" width="380" height="225" alt="fig 1"></p>
<p align="center" class="styleArial"><strong>Fig. 1</strong> &nbsp;Bias: estimator <img src="../../images/lectures/lecture9/theta1hat.gif" alt="theta1 hat" width="20" height="32" align="absmiddle"> is unbiased for &theta;, while estimator <img src="../../images/lectures/lecture9/theta2hat.gif" alt="theta2 hat" width="22" height="32" align="absmiddle"> is biased for &theta;. (<a href="https://sakai.unc.edu/access/content/group/2842013b-58f5-4453-aa8d-3e01bacbfc3d/public/Ecol562_Spring2012/notes/lecture9fig1.txt">R code</a>)</p>
<p>The bias of maximum likelihood estimates, when present, tends to disappear as the sample size increases. Thus MLEs are said to be asymptotically unbiased. </p>
<h3><a name="OLS"></a>Maximum likelihood estimation versus ordinary least squares</h3>
<p>To illustrate the connection between MLE and OLS  consider the problem of obtaining maximum likelihood estimates of the parameters of a simple linear regression model in which  the response is assumed to be normally distributed. The model in question is the following.</p>
<p align="center"><img src="../../images/lectures/lecture9/normalmodel.gif" width="168" height="67" alt="normal model"></p>
<p>If we have a sample of size <em>n</em>, the likelihood of this model, using R notation, is the following.</p>
<p align="center"><img src="../../images/lectures/lecture9/normlike.gif" width="457" height="57" alt="normal likelihood"></p>
<p>and the log-likelihood is</p>
<p align="center"><img src="../../images/lectures/lecture9/normloglike.gif" width="523" height="58" alt="normal LL"></p>
<p>To find the maximum likelihood estimates by hand we could use calculus. Calculus tells us that interior maxima occur at stationary points, i.e., points where all of the partial derivatives of our objective function are zero. Thus we could calculate the  partial derivatives of the log-likelihood with respect to the three parameters, set them equal to zero, and try to solve them for the individual parameters.</p>
<p align="center"><img src="../../images/lectures/lecture9/derivatives.gif" width="202" height="168" alt="partial derivatives"></p>
<p>In doing this for the normal model  a number of terms drop out. In the end maximizing the original log-likelihood   simplifies to  maximizing the following quantity:</p>
<p align="center"><img src="../../images/lectures/lecture9/negSSE.gif" width="168" height="58" alt="negative SSE"></p>
<p>This is the negative  SSE, the sum of squared errors that is minimized in ordinary least squares. So, maximizing the log-likelihood is the same as  maximizing &ndash;SSE which is the same as minimizing SSE. The upstart is that in the case of the normal regression model, maximum likelihood estimation and ordinary least squares yield the same solutions for the regression parameters.</p>
<p>Ordinary least squares does not directly yield an estimate for &sigma;, but  motivated by  the signal plus noise interpretation of ordinary least squares, the mean squared error of the model, MSE, is  a natural choice for estimating &sigma;<sup>2</sup>. The formula used is the following.</p>
<p align="center"><img src="../../images/lectures/lecture9/MSE.gif" width="217" height="85" alt="MSE"></p>
<p>where <em>p</em> is the number of estimated regression parameters and <em>n</em> is the sample size. For the simple linear regression example we've considering, <em>p</em> = 2, corresponding to the slope and intercept.</p>
<p>In the maximum likelihood approach when we solve the partial derivative equations for &sigma; we obtain a slightly different estimate of &sigma;<sup>2</sup>.</p>
<p align="center"><img src="../../images/lectures/lecture9/sigmamle.gif" width="220" height="82" alt="sigma2 MLE"></p>
<p>The two estimates differ in their choice of denominator. Because the MLE has a bigger denominator its estimate of &sigma; is slightly smaller. It turns out that the OLS estimate is unbiased (that's why it was chosen) whereas the MLE of &sigma; is biased. On the other hand the MLE of &sigma; is clearly asymptotically unbiased because when <em>n</em> is big, the difference between <em>n</em> and <em>n</em> &ndash; <em>p</em> is small and the effect on the estimate will be negligible.</p>
<h2><a name="significance"></a>Significance testing and maximum likelihood estimation</h2>
<p>In ordinary least squares there are two kinds of statistical tests that are useful in model simplification: the partial <em>F</em>-tests that are returned by the <span class="style1">anova</span> function of R and the individual <em>t</em>-tests that appear in the coefficients table produced by the <span class="style1">summary</span> function of R. Each of these tests has an equivalent version in the likelihood-based framework. The partial <em>F</em>-tests are replaced by likelihood ratio tests and the individual <em>t</em>-tests are replaced by Wald tests. We consider each of these in turn.</p>
<h3><a name="likelihood"></a>Likelihood ratio tests</h3>
<p>Consider the following  three Poisson models that were fit last time.</p>
<p align="center"><img src="../../images/lectures/lecture9/poissonmodels.gif" width="703" height="122" alt="Poisson models"></p>
<p>Here <em>z</em><sub>2</sub>, <em>z</em><sub>3</sub>, and <em>z</em><sub>4</sub> are dummy variables indicating weeks 2, 3, and 4 respectively.</p>
<div class="style23" style="padding-left: 30px; text-indent:-30px">rikz &lt;- read.table('ecol 562/RIKZ.txt', header=TRUE)</div>
<div class="style23" style="padding-left: 30px; text-indent:-30px"> rikz$richness &lt;- apply(rikz[,2:76], 1, function(x) sum(x&gt;0))</div>

<div class="style15" style="padding-left: 30px; text-indent:-30px">#Poisson models</div>
<div class="style23" style="padding-left: 30px; text-indent:-30px">mod1p &lt;- glm(richness~NAP, data=rikz, family=poisson)</div>
<div class="style23" style="padding-left: 30px; text-indent:-30px">mod2p &lt;- glm(richness~NAP+factor(week), data=rikz, family=poisson)</div>
<div class="style23" style="padding-left: 30px; text-indent:-30px">mod3p &lt;- glm(richness~NAP*factor(week), data=rikz, family=poisson)</div>
<p>Beginning with the most complex model we should first test whether the interaction terms are necessary thus determining if the coefficient of NAP varied by week. The corresponding hypothesis test is the following.</p>
<blockquote>
  <p>H<sub>0</sub>: &beta;<sub>5</sub> = &beta;<sub>6</sub> = &beta;<sub>7</sub> = 0<br>
    H<sub>1</sub>: at least one of &beta;<sub>5</sub>, &beta;<sub>6</sub>, &beta;<sub>7</sub> is not zero
  </p>
</blockquote>
<p>This hypothesis tests whether  model <span class="style8">mod3p</span> can be reduced to model <span class="style8">mod2p</span>. When two nested  models are fit by maximum likelihood  they can be compared using what's called a likelihood ratio test. The test statistic is the following.</p>
<p align="center"><img src="../../images/lectures/lecture9/LRstat.gif" width="310" height="100" alt="LR statistic"></p>
<p>where log <em>L</em>(<span class="style8">mod3</span>) and log<em> L</em>(<span class="style8">mod2</span>) are the log-likelihoods of <span class="style8">mod3</span> and <span class="style8">mod2</span> respectively. Notice that the larger, more complicated model appears in the numerator. The second equality shown in the above expression follows from a property of logarithms: the logarithm of a quotient is the difference of two logs.</p>
<p>The likelihood ratio statistic has a large sample chi-squared distribution with degrees of freedom given by the difference in the number of parameters in the two models. Put another way the degrees of freedom is the number of parameters we need to eliminate in the larger model to turn it into the smaller model.</p>
<p align="center"><img src="../../images/lectures/lecture9/chisquared.gif" width="233" height="63" alt="chisquared"></p>
<p align="center">where df = #parameters in mod3 &ndash; #parameters in mod2</p>
<p><a name="logLik" id="logLik"></a>The likelihood ratio test is a one-sided upper-tailed test. We reject the null hypothesis if the LR statistic is too big. We can extract the log-likelihood of a model in R with the <span class="style1">logLik</span> function.</p>
<div class="style23" style="padding-left: 30px; text-indent:-30px"> logLik(mod2p)</div>
<span class="style24">  'log Lik.' -97.73222 (df=5)</span>
<div class="style23" style="padding-left: 30px; text-indent:-30px"> logLik(mod3p)</div>
<span class="style24">  'log Lik.' -86.53437 (df=8)</span>
</p>
<p> The quantities in parentheses that appear after the log-likelihood and are labeled <span class="style8">df</span> are the number of  parameters that were estimated in each model. <a name="attr"></a>The number of estimated parameters, <span class="style8">df</span>, is an attribute of the <span class="style1">logLik</span> object and can be extracted with the <span class="style1">attr</span> function as follows.</p>
<div class="style15" style="padding-left: 30px; text-indent:-30px"> #number of estimated parameters</div>
<div class="style23" style="padding-left: 30px; text-indent:-30px"> attr(logLik(mod2p),'df')</div>
<span class="style24">  [1] 5</span>
 <div class="style23" style="padding-left: 30px; text-indent:-30px"> attr(logLik(mod3p),'df')</div>
 <span class="style24"> [1] 8</span>
 <p>The quantities needed for  the  likelihood ratio test are calculated as follows.</p>
 <div class="style15" style="padding-left: 30px; text-indent:-30px"> #likelihood ratio statistic</div>
 <div class="style23" style="padding-left: 30px; text-indent:-30px"> LR.stat &lt;- 2*(logLik(mod3p) - logLik(mod2p))</div>
 <div class="style23" style="padding-left: 30px; text-indent:-30px"> LR.stat</div>
 <span class="style24"> [1] 22.3957</span>

 <div class="style15" style="padding-left: 30px; text-indent:-30px"> #degrees of freedom for test, number of parameters dropped</div>
<div class="style23" style="padding-left: 30px; text-indent:-30px"> df.LR &lt;- attr(logLik(mod3p),'df') - attr(logLik(mod2p),'df')</div>
<div class="style23" style="padding-left: 30px; text-indent:-30px"> df.LR</div>
<span class="style24">  [1] 3</span>

<p><a name="pchisq"></a>The <em>p</em>-value of the test statistic is <img src="../../images/lectures/lecture9/pval.gif" alt="p-value" width="107" height="37" align="absmiddle">. It can be obtained using the <span class="style1">pchisq</span> function which calculates P(<em>X</em> &le; <em>k</em>) where <em>X</em> has a chi-squared distribution.</p>

<div class="style15" style="padding-left: 30px; text-indent:-30px"> #p-value for test statistic</div>
<div class="style23" style="padding-left: 30px; text-indent:-30px"> 1-pchisq(LR.stat, df.LR)</div>
<span class="style24">  [1] 5.396241e-05</span>

<p><a name="qchisq"></a>Because the <em>p</em>-value is small (less than .05) we reject the null hypothesis and conclude that  one or more interaction term coefficients  are different from zero and hence should be retained. Instead of calculating a <em>p</em>-value we could  calculate the critical value of our test using the <span class="style1">qchisq</span> function.<br>

<div class="style15" style="padding-left: 30px; text-indent:-30px"> #critical value of LR test: reject if LR.stat exceeds this</div>
<div class="style23" style="padding-left: 30px; text-indent:-30px"> qchisq(.95, df.LR)</div>
<span class="style24">[1] 7.814728</span>
<p>Because the observed value of the likelihood ratio statistic, 22.4, exceeds the critical value of 7.81, we  reject the null hypothesis.</p>
<p>It is unnecessary to carry out the likelihood ratio test by hand because the <span class="style1">anova</span> function of R will do it for us. If we give <span class="style1">anova</span> two nested models and specify <span class="style22">test='Chisq'</span> it will carry out a likelihood ratio test to see if the more complicated model can be reduced to the simpler model.</p>
<div class="style15" style="padding-left: 30px; text-indent:-30px"> #compare two nested models directly</div>
<div class="style23" style="padding-left: 30px; text-indent:-30px"> anova(mod2p, mod3p, test='Chisq')</div>
<span class="style24">Analysis of Deviance Table</span>
<p><span class="style24">Model 1: richness ~ NAP + factor(week)<br>
  Model 2: richness ~ NAP * factor(week)<br>
  &nbsp; Resid. Df Resid. Dev Df Deviance&nbsp; Pr(&gt;Chi)&nbsp;&nbsp;&nbsp; <br>
  1&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 40&nbsp;&nbsp;&nbsp;&nbsp; 53.466&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <br>
  2&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 37&nbsp;&nbsp;&nbsp;&nbsp; 31.070&nbsp; 3&nbsp;&nbsp; </span><span class="style25">22.396 5.396e-05</span><span class="style24"> ***<br>
  ---<br>
  Signif. codes:&nbsp; 0 &lsquo;***&rsquo; 0.001 &lsquo;**&rsquo; 0.01 &lsquo;*&rsquo; 0.05 &lsquo;.&rsquo; 0.1 &lsquo; &rsquo; 1</span>
<p>Observe that the test statistic, labeled <span class="style8">Deviance</span>, and the <em>p</em>-value, labeled <span class="style8">PR(&gt;Chi)</span>, are identical to what we calculated above.</p>
<p>Just as with normal models, using the <span class="style1">anova</span> function on a single model estimated using maximum likelihood causes R to carry out a sequence of tests. These tests will be  likelihood ratio tests if <span class="style22">test='Chisq'</span> is also specified.</p>
<div class="style23" style="padding-left: 30px; text-indent:-30px"> anova(mod3p, test='Chisq')</div>
<span class="style24">Analysis of Deviance Table</span>
<p><span class="style24">Model: poisson, link: log</span>
<p><span class="style24">Response: richness</span>
<p><span class="style24">Terms added sequentially (first to last)</span>

<p><span class="style24">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Df Deviance Resid. Df Resid. Dev&nbsp; Pr(&gt;Chi)&nbsp;&nbsp;&nbsp; <br>
  NULL&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 44&nbsp;&nbsp;&nbsp; 179.753&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <br>
  NAP&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 1&nbsp;&nbsp; 66.571&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 43&nbsp;&nbsp;&nbsp; 113.182 3.376e-16 ***<br>
  factor(week)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 3&nbsp;&nbsp; 59.716&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 40&nbsp;&nbsp;&nbsp;&nbsp; 53.466 6.759e-13 ***<br>
  NAP:factor(week)&nbsp; 3&nbsp;&nbsp; 22.396&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 37&nbsp;&nbsp;&nbsp;&nbsp; 31.070 5.396e-05 ***<br>
  ---<br>
  Signif. codes:&nbsp; 0 &lsquo;***&rsquo; 0.001 &lsquo;**&rsquo; 0.01 &lsquo;*&rsquo; 0.05 &lsquo;.&rsquo; 0.1 &lsquo; &rsquo; 1</span>
<p>The column labeled <span class="style8">Deviance</span> reports the likelihood ratio statistics for various tests. The three tests  carried out are the following.</p>
<ol>
  <li>Row 2 labeled <span class="style8">NAP</span> compares a model with NAP in it (<span class="style8">mod1p</span>) to a model with only an intercept. This is a 1 degree of freedom test (corresponding to the single coefficient of NAP) and tests the null hypothesis that the coefficient of NAP is equal to zero. The small reported <em>p</em>-value indicates that we should reject the null hypothesis.</li>
  <li>Row 3 labeled <span class="style8">factor(week)</span> compares a model with both NAP and factor(week) in it (<span class="style8">mod2p</span>) to a model with only NAP (<span class="style8">mod1p</span>). This is a 3 degrees of freedom test because there are three dummy variables for week. It tests the null hypothesis that the coefficients of all three dummy variables are zero. The small <em>p</em>-value indicates that we should reject this null hypothesis.</li>
  <li>Row 4 labeled <span class="style8">NAP:factor(week)</span> compares the interaction model (<span class="style8">mod3p</span>) to the additive model (<span class="style8">mod2p</span>). This is the same test that we carried out above with <span class="style11">anova(mod2p, mod3p, test='Chisq')</span>.</li>
</ol>
<h3><a name="wald"></a>Wald tests</h3>
<p> Wald tests are reported in the  coefficients table that is produced when the <span class="style1">summary</span> function is applied to a <span class="style12">glm</span> model. Like the <em>t</em>-tests shown in the summary output of an <span class="style12">lm</span> object, Wald tests are variables-added last tests, thus not all of the tests reported in the coefficients table will necessarily make  sense. When there are categorical variables in a model the Wald tests are useful for determining which of the groups are different.</p>
<div class="style23" style="padding-left: 30px; text-indent:-30px"> printCoefmat(summary(mod3p)$coefficients)</div>
<span class="style24">  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Estimate Std. Error z value&nbsp; Pr(&gt;|z|)&nbsp;&nbsp;&nbsp; <br>
  (Intercept)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.423838&nbsp;&nbsp; 0.095932 25.2661 &lt; 2.2e-16 ***<br>
  NAP&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -0.162908&nbsp;&nbsp; 0.104308 -1.5618&nbsp;&nbsp; 0.11834&nbsp;&nbsp;&nbsp; <br>
  factor(week)2&nbsp;&nbsp;&nbsp;&nbsp; -1.298001&nbsp;&nbsp; 0.185045 -7.0145 2.307e-12 ***<br>
  factor(week)3&nbsp;&nbsp;&nbsp;&nbsp; -0.910691&nbsp;&nbsp; 0.159247 -5.7187 1.073e-08 ***<br>
  factor(week)4&nbsp;&nbsp;&nbsp;&nbsp; -0.099272&nbsp;&nbsp; 0.191739 -0.5177&nbsp;&nbsp; 0.60464&nbsp;&nbsp;&nbsp; <br>
  NAP:factor(week)2 -0.425546&nbsp;&nbsp; 0.219664 -1.9373&nbsp;&nbsp; 0.05271 .&nbsp; <br>
  NAP:factor(week)3 -0.404259&nbsp;&nbsp; 0.181650 -2.2255&nbsp;&nbsp; 0.02605 *&nbsp; <br>
  NAP:factor(week)4 -1.230126&nbsp;&nbsp; 0.301916 -4.0744 4.613e-05 ***<br>
  ---<br>
Signif. codes:&nbsp; 0 &lsquo;***&rsquo; 0.001 &lsquo;**&rsquo; 0.01 &lsquo;*&rsquo; 0.05 &lsquo;.&rsquo; 0.1 &lsquo; &rsquo; 1</span>
<p>From the output we see that coefficient of NAP is negative but not significantly different from zero in week 1 (<em>p</em> = 0118). The three interaction terms then test if the coefficients of NAP in weeks 2, 3, and 4 are different from the value in week 1. From the output the coefficient of NAP in week 4 is certainly different from what it is in week 1. For weeks 2 and 3 the results are less clear: the slope in week 3 is significantly different (<em>p</em> = 0.026) but the slope in week 2 just misses being significantly different (<em>p</em> = 0.053). The estimates of the differences are all negative indicating that the coefficients of NAP in weeks 2, 3, and 4 are even more negative than they are in week 1.</p>
<p>Because Wald tests and likelihood ratio tests make different assumptions about the likelihood, the two tests need not  agree even when they are testing the same hypothesis. Both are  large-sample tests but the general recommendation is that when sample sizes are small, the results from likelihood ratio tests tend to be more trustworthy than the results from Wald tests.</p>
<h3><a name="waldci"></a>Wald-based Confidence intervals</h3>
<p>The maximum likelihood estimates of  regression parameters are asymptotically normal, thus we can construct normal-based Wald confidence intervals for the parameters using the  estimates and  standard errors reported in the  summary table output. The summary coefficient table is just a matrix so we can access its entries by row and column number. For example, the calculation below obtains the 95% confidence interval for the coefficient of NAP.</p>
<div class="style15" style="padding-left: 30px; text-indent:-30px">#lower limit</div>
<div class="style23" style="padding-left: 30px; text-indent:-30px"> summary(mod3p)$coefficients[2,1] + qnorm(.025) * summary(mod3p)$coefficients[2,2]</div>
<span class="style24">  [1] -0.3673486</span>
<div class="style15" style="padding-left: 30px; text-indent:-30px">  #upper limit
</div>
<div class="style23" style="padding-left: 30px; text-indent:-30px"> summary(mod3p)$coefficients[2,1] + qnorm(.975) * summary(mod3p)$coefficients[2,2]</div>
<span class="style24">[1] 0.0415321</span>
<p>If we leave off the row numbers in the above calculation we can obtain 95% confidence intervals for all of the estimates in the summary table.</p>
<div class="style23" style="padding-left: 30px; text-indent:-30px">data.frame(lower95=summary(mod3p)$coefficients[,1] + qnorm(.025) * summary(mod3p)$coefficients[,2], upper95=
  summary(mod3p)$coefficients[,1] + qnorm(.975) * summary(mod3p)$coefficients[,2])</div>
<span class="style24">  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; lower95&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;upper95<br>
  (Intercept)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.2358146&nbsp; 2.611862249<br>
  NAP&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -0.3673486&nbsp; 0.041532099<br>
  factor(week)2&nbsp;&nbsp;&nbsp;&nbsp; -1.6606825 -0.935320315<br>
  factor(week)3&nbsp;&nbsp;&nbsp;&nbsp; -1.2228097 -0.598572020<br>
  factor(week)4&nbsp;&nbsp;&nbsp;&nbsp; -0.4750731&nbsp; 0.276529053<br>
  NAP:factor(week)2 -0.8560783&nbsp; 0.004987289<br>
  NAP:factor(week)3 -0.7602866 -0.048232032<br>
  NAP:factor(week)4 -1.8218702 -0.638381906</span>
<h3><a name="lrci"></a>Likelihood-based confidence intervals</h3>
<p>Confidence intervals can also be constructed based on the likelihood ratio test. These are called profile likelihood confidence intervals and are calculated when the <span class="style1">confint</span> function is applied to a <span class="style1">glm</span> model object.</p>
<div class="style23" style="padding-left: 30px; text-indent:-30px"> confint(mod3p)</div>
<span class="style24">  Waiting for profiling to be done...<br>
  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.5 %&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 97.5 %<br>
  (Intercept)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.2293442&nbsp; 2.60585532<br>
  NAP&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; -0.3682098&nbsp; 0.04152792<br>
  factor(week)2&nbsp;&nbsp;&nbsp;&nbsp; -1.6718867 -0.94433406<br>
  factor(week)3&nbsp;&nbsp;&nbsp;&nbsp; -1.2280073 -0.60234216<br>
  factor(week)4&nbsp;&nbsp;&nbsp;&nbsp; -0.4884212&nbsp; 0.26581518<br>
  NAP:factor(week)2 -0.8674195 -0.00341266<br>
  NAP:factor(week)3 -0.7722811 -0.05792222<br>
NAP:factor(week)4 -1.8611611 -0.66819916</span>
<p>In this case the results we obtain are very close to the Wald-based results. </p>
<h2 align="left"><a name="model"></a>Model selection criteria</h2>
<p>Significance testing can be used to choose between models but only when those models are nested. What about comparing models that are not nested or comparing  models that use different probability models? For instance suppose we fit the same model to data but in one case we assume the response has a Poisson distribution and in the other case we assume it is normally distributed.</p>
<table width="550" border="0" align="center" cellpadding="0">
  <tr>
    <td><img src="../../images/lectures/lecture9/Poisson1.gif" width="235" height="62" alt="Poisson"></td>
    <td><img src="../../images/lectures/lecture9/Normal1.gif" width="218" height="67" alt="normal"></td>
  </tr>
</table>
<div align="center"></div>
<p>For count data if we interpret the normal densities in the likelihood for the normal model as being midpoint approximations of the probabilities of the individual counts, then we can compare these two models directly using log-likelihood (see <a href="lecture8.htm#comparing">lecture 8</a>). The model with the larger log-likelihood is the one that makes our data more probable.</p>
<div class="style23" style="padding-left: 30px; text-indent:-30px"> mod1p &lt;- glm(richness~NAP, data=rikz, family=poisson)</div>
<div class="style23" style="padding-left: 30px; text-indent:-30px"> mod1 &lt;- lm(richness~NAP, data=rikz)</div>
<div class="style23" style="padding-left: 30px; text-indent:-30px"> logLik(mod1p)</div>
 <span class="style24"> 'log Lik.' -127.5904 (df=2)</span>
<div class="style23" style="padding-left: 30px; text-indent:-30px">logLik(mod1)</div>
<span class="style24">'log Lik.' -126.9767 (df=3)</span>
<p>Based on the reported log-likelihoods the data are more probable (greater log-likelihood) under the normal model than under the Poisson model. </p>
<p>In truth this isn't a fair comparison. The normal model uses three parameters: &beta;<sub>0</sub>, &beta;<sub>1</sub>, and &sigma;<sup>2</sup> whereas the Poisson model uses just two. It is easier to fit data when one has more parameters at one's disposal. John von Neumann is famously reported to have said, &quot;With four parameters I can fit an elephant, but with five I can make him wiggle his trunk.&quot;  The log-likelihood can never decrease as one adds additional parameters to a given probability model. At worse, it will remain the same. So perhaps a better strategy is to penalize a model that fits better but does so at the cost of estimating more parameters.</p>
<p>There have been many ad hoc methods proposed to penalize models that have become too complex. In ordinary linear regression two such criteria are adjusted R<sup>2</sup> and Mallow's C<sub>p</sub>.</p>
<p align="center">adjusted R<sup>2</sup>: <img src="../../images/lectures/lecture9/R2adj.gif" alt="R2adj" width="187" height="55" align="absmiddle"></p>
<p align="center">Mallow's C<sub>p</sub>: <img src="../../images/lectures/lecture9/cp.gif" alt="Cp" width="162" height="53" align="absmiddle"></p>
<p>Here <em>n</em> is the sample size, <em>p</em> is the number of parameters in the model, s<sup>2</sup> = MSE for the full model (i.e., is the model containing all <em>k</em> explanatory variables of interest), SSE<sub>p</sub> is the residual sum of squares for the subset model containing <em>p</em> parameters including the intercept, and R<sup>2</sup> is the ordinary R<sup>2</sup> statistic (coefficient of determination). Both  adjusted R<sup>2</sup> and Mallow's C<sub>p</sub>  end up penalizing those models that include parameters that contribute very little explanatory power to the model.</p>
<p>Penalized measures also exist for likelihood-based models  where they are referred to as information-theoretic criteria. Three popular ones are AIC, AIC<sub>c</sub>, and BIC.</p>
<p align="center"><img src="../../images/lectures/lecture9/informationcriteria.gif" width="265" height="112"></p>
<p>Here log <em>L</em> is the log-likelihood of the model, <em>k</em> is the number of parameters that were estimated, and <em>n</em> is the sample size. AIC<sub>c</sub> is  a small sample correction to AIC and is recommended when the ratio of <em>n</em> to <em>k</em> is less than 40, i.e., <img src="../../images/lectures/lecture9/ratop.gif" alt="ratio" width="70" height="38" align="absmiddle">. In other cases the difference between AIC and AIC<sub>c</sub> is negligible.</p>
<p>These three statistics differ in the way that they penalize complexity. AIC and its small sample alternative AIC<sub>c</sub> stand out because they are more than   ad hoc corrections to the log-likelihood. They also have a strong theoretical basis in information theory. We'll explore this theoretical basis next time.</p>
<p align="center"><a href="../../index.html">Course Home Page</a> </p>
<hr align="center" width="75%">
<!--Standard footer follows -->
<p></p>
<table width="650" border="3" cellspacing="2" cellpadding="2" align=
"CENTER">
  <tr bgcolor="#CCCCCC">
    <td width="100%"><font size=-1>Jack Weiss<br>
          <i>Phone: </i>(919) 962-5930<br>
          <i>E-Mail:</i> jack_weiss@unc.edu<br>
          <i>Address: </i>Curriculum for the Environment and Ecology, Box 3275, University of North Carolina, Chapel Hill, 27599<br>
      Copyright &copy; 2012<br>
      Last Revised--February 4, 2012<br>
      URL: <a href="lecture9.htm#lecture9" target="_self">https://sakai.unc.edu/access/content/group/2842013b-58f5-4453-aa8d-3e01bacbfc3d/public/Ecol562_Spring2012/docs/lectures/lecture9.htm</a></font></td>
  </tr>
</table>
<p align="center">&nbsp;</p>
</body>
</html>
